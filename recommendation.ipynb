{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccab289-3545-4116-8f80-31b3903f5fcd",
   "metadata": {},
   "source": [
    "# RecSys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb89cea-8698-4aa2-962b-dc06e592e7ef",
   "metadata": {},
   "source": [
    ">[!NOTE]\n",
    ">\n",
    ">I have tested some model some key points are:\n",
    ">1. no FE was done.\n",
    ">2. just fine tune some model and choose those model that perform good on the dataset.\n",
    ">3. The rsme will be higher ( The data is mostly random generated so based on that its hard to actually says the model perform well -- the random thing made the models hard to learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932f563-e419-47ce-99d0-21b42286eeae",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1544cb55-4361-4e37-b775-3a6f4cdf564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from score import score\n",
    "from rec_utils import split_the_data\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# surprise\n",
    "from surprise import KNNBasic\n",
    "from surprise import SVDpp, BaselineOnly\n",
    "from surprise.model_selection import train_test_split as sur_tts\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da859d77-87e0-4fc0-9b39-f8e5f3a495f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(\"../Course_Scraper/assets/augumented_data/augmented_user_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57a233e5-7193-42c4-8eac-85842f0d84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "course = pd.read_csv(\"../Course_Scraper/assets/formated_data/formated_course.csv\")[['title_index', 'skills']]\n",
    "courses = pd.read_csv(\"../Course_Scraper/assets/formated_data/formated_courses.csv\")[['title_index', 'skills']]\n",
    "\n",
    "course.dropna(subset='skills', inplace=True)\n",
    "courses.dropna(subset='skills', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a3e2e-b08a-4521-818b-f1ded0a846ac",
   "metadata": {},
   "source": [
    "--- get total skills that are unique so i can build something ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5d6262-7ff2-4e40-9688-e6a48a85f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(data):\n",
    "    \"\"\"\n",
    "    Single list or simply for row\n",
    "    \"\"\"\n",
    "    return ast.literal_eval(data)\n",
    "\n",
    "def convert_series_to_list(data: pd, warn=False, add_nan=True):\n",
    "    \"\"\"\n",
    "    Here data is the pd.Series\n",
    "    \"\"\"\n",
    "    return [skill for sublist in data for skill in sublist]\n",
    "\n",
    "def get_unique_skills(course, courses):\n",
    "    l1 = convert_series_to_list(course['skills'])\n",
    "    l2 = convert_series_to_list(courses['skills'])\n",
    "    l = l1 + l2\n",
    "    return set(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e6db9-af59-4806-9cd1-ae8da5b78fbf",
   "metadata": {},
   "source": [
    "--- building genres (one hot encoding) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d6a2aa6-9032-42c3-b660-6fc2cd4f0baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skills are: 1588\n"
     ]
    }
   ],
   "source": [
    "course['skills'] = course['skills'].apply(lambda x: convert_to_list(x))\n",
    "courses['skills'] = courses['skills'].apply(lambda x: convert_to_list(x))\n",
    "\n",
    "all_skills = list(get_unique_skills(course, courses))\n",
    "print(f'Total skills are: {len(all_skills)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c1db24d-8583-43f2-a8ac-557b8e783054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skills = pd.concat([course.explode('skills'), courses.explode('skills')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550fc0f4-fda9-48ac-a010-189770de8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_onehot = pd.get_dummies(skills).astype(int).groupby('title_index').max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6aab798-0ee4-4838-82ed-7e89f037c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = user_data.merge(skill_onehot, on='title_index', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "551efcc9-36e3-493d-beb7-4498652ad696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_index</th>\n",
       "      <th>user_index</th>\n",
       "      <th>rating</th>\n",
       "      <th>skills_3D Assets</th>\n",
       "      <th>skills_A/B Testing</th>\n",
       "      <th>skills_AI Personalization</th>\n",
       "      <th>skills_AI Product Strategy</th>\n",
       "      <th>skills_AWS CloudFormation</th>\n",
       "      <th>skills_AWS Identity and Access Management (IAM)</th>\n",
       "      <th>skills_AWS Kinesis</th>\n",
       "      <th>...</th>\n",
       "      <th>skills_Writing</th>\n",
       "      <th>skills_Writing and Editing</th>\n",
       "      <th>skills_X-Ray Computed Tomography</th>\n",
       "      <th>skills_and Compliance</th>\n",
       "      <th>skills_and Development</th>\n",
       "      <th>skills_and Facility Services</th>\n",
       "      <th>skills_and Logistics</th>\n",
       "      <th>skills_and Social Studies</th>\n",
       "      <th>skills_and Technical Instruments</th>\n",
       "      <th>skills_iOS Development</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21103</th>\n",
       "      <td>48</td>\n",
       "      <td>4238</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21104</th>\n",
       "      <td>48</td>\n",
       "      <td>2283</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21105</th>\n",
       "      <td>48</td>\n",
       "      <td>6596</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21106</th>\n",
       "      <td>48</td>\n",
       "      <td>4798</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21107</th>\n",
       "      <td>48</td>\n",
       "      <td>5125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title_index  user_index  rating  skills_3D Assets  skills_A/B Testing  \\\n",
       "21103           48        4238       1               0.0                 0.0   \n",
       "21104           48        2283       1               0.0                 0.0   \n",
       "21105           48        6596       4               0.0                 0.0   \n",
       "21106           48        4798       3               0.0                 0.0   \n",
       "21107           48        5125       1               0.0                 0.0   \n",
       "\n",
       "       skills_AI Personalization  skills_AI Product Strategy  \\\n",
       "21103                        0.0                         0.0   \n",
       "21104                        0.0                         0.0   \n",
       "21105                        0.0                         0.0   \n",
       "21106                        0.0                         0.0   \n",
       "21107                        0.0                         0.0   \n",
       "\n",
       "       skills_AWS CloudFormation  \\\n",
       "21103                        0.0   \n",
       "21104                        0.0   \n",
       "21105                        0.0   \n",
       "21106                        0.0   \n",
       "21107                        0.0   \n",
       "\n",
       "       skills_AWS Identity and Access Management (IAM)  skills_AWS Kinesis  \\\n",
       "21103                                              0.0                 0.0   \n",
       "21104                                              0.0                 0.0   \n",
       "21105                                              0.0                 0.0   \n",
       "21106                                              0.0                 0.0   \n",
       "21107                                              0.0                 0.0   \n",
       "\n",
       "       ...  skills_Writing  skills_Writing and Editing  \\\n",
       "21103  ...             0.0                         0.0   \n",
       "21104  ...             0.0                         0.0   \n",
       "21105  ...             0.0                         0.0   \n",
       "21106  ...             0.0                         0.0   \n",
       "21107  ...             0.0                         0.0   \n",
       "\n",
       "       skills_X-Ray Computed Tomography  skills_and Compliance  \\\n",
       "21103                               0.0                    0.0   \n",
       "21104                               0.0                    0.0   \n",
       "21105                               0.0                    0.0   \n",
       "21106                               0.0                    0.0   \n",
       "21107                               0.0                    0.0   \n",
       "\n",
       "       skills_and Development  skills_and Facility Services  \\\n",
       "21103                     0.0                           0.0   \n",
       "21104                     0.0                           0.0   \n",
       "21105                     0.0                           0.0   \n",
       "21106                     0.0                           0.0   \n",
       "21107                     0.0                           0.0   \n",
       "\n",
       "       skills_and Logistics  skills_and Social Studies  \\\n",
       "21103                   0.0                        0.0   \n",
       "21104                   0.0                        0.0   \n",
       "21105                   0.0                        0.0   \n",
       "21106                   0.0                        0.0   \n",
       "21107                   0.0                        0.0   \n",
       "\n",
       "       skills_and Technical Instruments  skills_iOS Development  \n",
       "21103                               0.0                     0.0  \n",
       "21104                               0.0                     0.0  \n",
       "21105                               0.0                     0.0  \n",
       "21106                               0.0                     0.0  \n",
       "21107                               0.0                     0.0  \n",
       "\n",
       "[5 rows x 1591 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d94f7a-8d5a-4a00-99a3-7267ce2e09a7",
   "metadata": {},
   "source": [
    "--- mapping just in case in need it ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2fb91f1-903d-4675-93e5-e313f996ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = {\n",
    "    k: x for x, k in enumerate(user_data['user_index'].unique())\n",
    "}\n",
    "\n",
    "# so we get a consistent titles\n",
    "title_mapping = {\n",
    "    k: x for x, k in enumerate(user_data['title_index'].unique())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35097c95-e02b-432d-8d97-4536196b86b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e0a08-2108-404d-9220-c4ee219c7fda",
   "metadata": {},
   "source": [
    "## CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ecbf2d5-3bb3-46fc-8440-5d59c462f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configs\n",
    "class CONFIG:\n",
    "    seed = 67\n",
    "    BATCH_SIZE = 4096\n",
    "    EPOCH = 1500\n",
    "    \n",
    "cfg = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d40fef4-7f2e-427a-881b-9f849ef2f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8379766ef0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ccfab-c52f-4b4e-a37e-f5d300f0b911",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823f0a8-76a1-403a-8309-d7305736d5c0",
   "metadata": {},
   "source": [
    "## Surpise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244be728-e2fa-4191-a62d-1f406bb77f64",
   "metadata": {},
   "source": [
    "#### BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b03599-5f27-4200-8d9c-c9c84643d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "RMSE: 1.6295\n",
      "\n",
      "Socre on Test:  1.6295493210121377\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(user_data[['user_index', 'title_index', 'rating']], reader)\n",
    "train, test = sur_tts(data, test_size=0.2, shuffle=True)\n",
    "\n",
    "bsl_options = {\n",
    "    \"n_epochs\": 20,\n",
    "    \"method\": \"sgd\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"seed\": cfg.seed\n",
    "}\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "algo.fit(train)\n",
    "predictions = algo.test(test)\n",
    "print(\"\\nSocre on Test: \", accuracy.rmse(predictions, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61f9b1-cd4d-42ba-a13a-afdae5562329",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fec46e52-eee5-453b-ab81-dffb419d6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "knn = KNNBasic(sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eb11660-0888-4618-bcf2-9f2c8713ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.7433\n",
      "\n",
      "Socre on Test:  1.743301745847061\n"
     ]
    }
   ],
   "source": [
    "knn.fit(train)\n",
    "predictions = knn.test(test)\n",
    "print(\"\\nSocre on Test: \", accuracy.rmse(predictions, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ea12e-8192-4355-8505-69f13c0df736",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9faef2-70dc-4a0a-8799-71e675638639",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "918567ae-6542-49b7-9dc1-33d5f2df7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['user_index'] = user_data['user_index'].map(user_mapping)\n",
    "user_data['title_index'] = user_data['title_index'].map(title_mapping)\n",
    "\n",
    "# data, test = split_the_data(user_data)\n",
    "data, test = split_the_data(user_data[['title_index', 'user_index', 'rating']])\n",
    "train, val = split_the_data(data)\n",
    "\n",
    "X_train = train.drop('rating', axis=1)\n",
    "X_val = val.drop('rating', axis=1)\n",
    "\n",
    "y_train = train['rating']\n",
    "y_val = val['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cdf403c-804f-4f06-8819-4b8122916dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 10632\n",
      "Total titles: 548\n"
     ]
    }
   ],
   "source": [
    "total_users = len(user_mapping)\n",
    "total_titles = len(title_mapping)\n",
    "\n",
    "print(f'Total users: {total_users}')\n",
    "print(f'Total titles: {total_titles}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f5f19b3-6586-466c-aef9-3e040e6ee950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(features, features)\n",
    "        self.bn1 = nn.BatchNorm1d(features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(features, features)\n",
    "        self.bn2 = nn.BatchNorm1d(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.fc1(x)))\n",
    "        out = self.bn2(self.fc2(out))\n",
    "        out += identity \n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c8954a9-687d-4def-82ee-37882cd93e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thought of adding attention in it, but though...................meh...just made it run\n",
    "class RecSys(nn.Module):\n",
    "    def __init__(self, total_users, total_tiltes, user_emb_dim: int = 32, title_emb_dim: int = 32): # skill_emb_dim=32,  total_skills\n",
    "        super(RecSys, self).__init__()\n",
    "        self.usr_emb = nn.Embedding(total_users, user_emb_dim)\n",
    "        self.title_emb = nn.Embedding(total_titles, title_emb_dim)\n",
    "        # self.skill_emb = nn.Embedding(total_skills, skill_emb_dim)\n",
    "        self.fc1 = nn.Linear((user_emb_dim + title_emb_dim), 128)\n",
    "        # self.skill_proj = nn.Linear(total_skills, skill_emb_dim, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.res_block1 = ResidualBlock(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.res_block2 = ResidualBlock(64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        user = x[:, 1].long()\n",
    "        title = x[:, 0].long()\n",
    "        # skill = x[:, 2:].float()\n",
    "\n",
    "        u = self.usr_emb(user)\n",
    "        t = self.title_emb(title)\n",
    "        # s = self.skill_proj(skill.float())\n",
    "        \n",
    "        # x = torch.concat([u, t, s], dim=1)\n",
    "        x = torch.concat([u, t], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.res_block1(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.res_block2(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4a8d2ee-00bc-4ff4-a75e-a8d9439be2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RecSys(total_users=total_users, total_tiltes=total_titles, total_skills=len(all_skills)).to('cuda')\n",
    "model = RecSys(total_users=total_users, total_tiltes=total_titles).to('cuda')\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-2, momentum=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30c7af87-4c21-4407-8904-53cdd86d47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_Xtr = torch.tensor(X_train.to_numpy(), dtype=torch.float)  \n",
    "tensor_Xva = torch.tensor(X_val.to_numpy(), dtype=torch.float)    \n",
    "tensor_ytr = torch.tensor(y_train.to_numpy(), dtype=torch.float)  \n",
    "tensor_yva = torch.tensor(y_val.to_numpy(), dtype=torch.float)    \n",
    "\n",
    "train_dataset = TensorDataset(tensor_Xtr, tensor_ytr)\n",
    "val_dataset   = TensorDataset(tensor_Xva, tensor_yva)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15e8fd37-1e93-4617-828e-e112e0d60165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 3.8464 - Val Loss: 3.7643\n",
      "Epoch 2 - Train Loss: 3.6638 - Val Loss: 3.6349\n",
      "Epoch 3 - Train Loss: 3.4368 - Val Loss: 3.4770\n",
      "Epoch 4 - Train Loss: 3.1996 - Val Loss: 3.2850\n",
      "Epoch 5 - Train Loss: 2.9842 - Val Loss: 3.0617\n",
      "Epoch 6 - Train Loss: 2.7732 - Val Loss: 2.8170\n",
      "Epoch 7 - Train Loss: 2.5822 - Val Loss: 2.5736\n",
      "Epoch 8 - Train Loss: 2.3929 - Val Loss: 2.3559\n",
      "Epoch 9 - Train Loss: 2.2418 - Val Loss: 2.1791\n",
      "Epoch 10 - Train Loss: 2.1080 - Val Loss: 2.0491\n",
      "Epoch 11 - Train Loss: 2.0075 - Val Loss: 1.9597\n",
      "Epoch 12 - Train Loss: 1.9283 - Val Loss: 1.9024\n",
      "Epoch 13 - Train Loss: 1.8712 - Val Loss: 1.8665\n",
      "Epoch 14 - Train Loss: 1.8342 - Val Loss: 1.8437\n",
      "Epoch 15 - Train Loss: 1.8063 - Val Loss: 1.8290\n",
      "Epoch 16 - Train Loss: 1.7894 - Val Loss: 1.8189\n",
      "Epoch 17 - Train Loss: 1.7787 - Val Loss: 1.8112\n",
      "Epoch 18 - Train Loss: 1.7742 - Val Loss: 1.8053\n",
      "Epoch 19 - Train Loss: 1.7585 - Val Loss: 1.8002\n",
      "Epoch 20 - Train Loss: 1.7518 - Val Loss: 1.7960\n",
      "Epoch 21 - Train Loss: 1.7531 - Val Loss: 1.7927\n",
      "Epoch 22 - Train Loss: 1.7428 - Val Loss: 1.7896\n",
      "Epoch 23 - Train Loss: 1.7425 - Val Loss: 1.7872\n",
      "Epoch 24 - Train Loss: 1.7426 - Val Loss: 1.7842\n",
      "Epoch 25 - Train Loss: 1.7415 - Val Loss: 1.7820\n",
      "Epoch 26 - Train Loss: 1.7315 - Val Loss: 1.7799\n",
      "Epoch 27 - Train Loss: 1.7288 - Val Loss: 1.7783\n",
      "Epoch 28 - Train Loss: 1.7270 - Val Loss: 1.7769\n",
      "Epoch 29 - Train Loss: 1.7275 - Val Loss: 1.7757\n",
      "Epoch 30 - Train Loss: 1.7210 - Val Loss: 1.7744\n",
      "Epoch 31 - Train Loss: 1.7202 - Val Loss: 1.7737\n",
      "Epoch 32 - Train Loss: 1.7127 - Val Loss: 1.7728\n",
      "Epoch 33 - Train Loss: 1.7139 - Val Loss: 1.7722\n",
      "Epoch 34 - Train Loss: 1.7160 - Val Loss: 1.7714\n",
      "Epoch 35 - Train Loss: 1.7129 - Val Loss: 1.7707\n",
      "Epoch 36 - Train Loss: 1.7203 - Val Loss: 1.7696\n",
      "Epoch 37 - Train Loss: 1.7167 - Val Loss: 1.7689\n",
      "Epoch 38 - Train Loss: 1.7144 - Val Loss: 1.7679\n",
      "Epoch 39 - Train Loss: 1.7057 - Val Loss: 1.7673\n",
      "Epoch 40 - Train Loss: 1.7070 - Val Loss: 1.7667\n",
      "Epoch 41 - Train Loss: 1.7063 - Val Loss: 1.7661\n",
      "Epoch 42 - Train Loss: 1.7088 - Val Loss: 1.7660\n",
      "Epoch 43 - Train Loss: 1.7009 - Val Loss: 1.7657\n",
      "Epoch 44 - Train Loss: 1.7067 - Val Loss: 1.7653\n",
      "Epoch 45 - Train Loss: 1.7088 - Val Loss: 1.7651\n",
      "Epoch 46 - Train Loss: 1.7096 - Val Loss: 1.7643\n",
      "Epoch 47 - Train Loss: 1.7040 - Val Loss: 1.7635\n",
      "Epoch 48 - Train Loss: 1.7063 - Val Loss: 1.7629\n",
      "Epoch 49 - Train Loss: 1.6994 - Val Loss: 1.7624\n",
      "Epoch 50 - Train Loss: 1.7084 - Val Loss: 1.7625\n",
      "Epoch 51 - Train Loss: 1.7007 - Val Loss: 1.7618\n",
      "Epoch 52 - Train Loss: 1.7035 - Val Loss: 1.7618\n",
      "Epoch 53 - Train Loss: 1.7005 - Val Loss: 1.7616\n",
      "Epoch 54 - Train Loss: 1.7023 - Val Loss: 1.7613\n",
      "Epoch 55 - Train Loss: 1.7030 - Val Loss: 1.7612\n",
      "Epoch 56 - Train Loss: 1.7026 - Val Loss: 1.7607\n",
      "Epoch 57 - Train Loss: 1.6997 - Val Loss: 1.7601\n",
      "Epoch 58 - Train Loss: 1.7019 - Val Loss: 1.7598\n",
      "Epoch 59 - Train Loss: 1.6985 - Val Loss: 1.7598\n",
      "Epoch 60 - Train Loss: 1.7020 - Val Loss: 1.7597\n",
      "Epoch 61 - Train Loss: 1.6943 - Val Loss: 1.7594\n",
      "Epoch 62 - Train Loss: 1.7050 - Val Loss: 1.7594\n",
      "Epoch 63 - Train Loss: 1.6986 - Val Loss: 1.7591\n",
      "Epoch 64 - Train Loss: 1.6968 - Val Loss: 1.7586\n",
      "Epoch 65 - Train Loss: 1.6976 - Val Loss: 1.7586\n",
      "Epoch 66 - Train Loss: 1.6900 - Val Loss: 1.7583\n",
      "Epoch 67 - Train Loss: 1.7000 - Val Loss: 1.7584\n",
      "Epoch 68 - Train Loss: 1.6891 - Val Loss: 1.7582\n",
      "Epoch 69 - Train Loss: 1.6901 - Val Loss: 1.7582\n",
      "Epoch 70 - Train Loss: 1.7029 - Val Loss: 1.7580\n",
      "Epoch 71 - Train Loss: 1.6949 - Val Loss: 1.7574\n",
      "Epoch 72 - Train Loss: 1.6987 - Val Loss: 1.7570\n",
      "Epoch 73 - Train Loss: 1.6933 - Val Loss: 1.7567\n",
      "Epoch 74 - Train Loss: 1.6985 - Val Loss: 1.7568\n",
      "Epoch 75 - Train Loss: 1.7012 - Val Loss: 1.7564\n",
      "Epoch 76 - Train Loss: 1.6971 - Val Loss: 1.7561\n",
      "Epoch 77 - Train Loss: 1.6926 - Val Loss: 1.7560\n",
      "Epoch 78 - Train Loss: 1.6947 - Val Loss: 1.7557\n",
      "Epoch 79 - Train Loss: 1.6943 - Val Loss: 1.7555\n",
      "Epoch 80 - Train Loss: 1.6874 - Val Loss: 1.7557\n",
      "Epoch 81 - Train Loss: 1.6907 - Val Loss: 1.7560\n",
      "Epoch 82 - Train Loss: 1.6920 - Val Loss: 1.7560\n",
      "Epoch 83 - Train Loss: 1.6942 - Val Loss: 1.7556\n",
      "Epoch 84 - Train Loss: 1.6891 - Val Loss: 1.7553\n",
      "Epoch 85 - Train Loss: 1.6909 - Val Loss: 1.7552\n",
      "Epoch 86 - Train Loss: 1.6914 - Val Loss: 1.7551\n",
      "Epoch 87 - Train Loss: 1.6897 - Val Loss: 1.7550\n",
      "Epoch 88 - Train Loss: 1.6904 - Val Loss: 1.7550\n",
      "Epoch 89 - Train Loss: 1.6905 - Val Loss: 1.7548\n",
      "Epoch 90 - Train Loss: 1.6903 - Val Loss: 1.7549\n",
      "Epoch 91 - Train Loss: 1.6934 - Val Loss: 1.7545\n",
      "Epoch 92 - Train Loss: 1.6984 - Val Loss: 1.7539\n",
      "Epoch 93 - Train Loss: 1.6934 - Val Loss: 1.7538\n",
      "Epoch 94 - Train Loss: 1.6862 - Val Loss: 1.7538\n",
      "Epoch 95 - Train Loss: 1.6927 - Val Loss: 1.7537\n",
      "Epoch 96 - Train Loss: 1.6914 - Val Loss: 1.7538\n",
      "Epoch 97 - Train Loss: 1.6920 - Val Loss: 1.7537\n",
      "Epoch 98 - Train Loss: 1.6853 - Val Loss: 1.7537\n",
      "Epoch 99 - Train Loss: 1.6821 - Val Loss: 1.7538\n",
      "Epoch 100 - Train Loss: 1.6860 - Val Loss: 1.7540\n",
      "Epoch 101 - Train Loss: 1.6906 - Val Loss: 1.7542\n",
      "Epoch 102 - Train Loss: 1.6866 - Val Loss: 1.7539\n",
      "Epoch 103 - Train Loss: 1.6864 - Val Loss: 1.7537\n",
      "Epoch 104 - Train Loss: 1.6905 - Val Loss: 1.7536\n",
      "Epoch 105 - Train Loss: 1.6856 - Val Loss: 1.7533\n",
      "Epoch 106 - Train Loss: 1.6847 - Val Loss: 1.7534\n",
      "Epoch 107 - Train Loss: 1.6920 - Val Loss: 1.7535\n",
      "Epoch 108 - Train Loss: 1.6841 - Val Loss: 1.7531\n",
      "Epoch 109 - Train Loss: 1.6883 - Val Loss: 1.7528\n",
      "Epoch 110 - Train Loss: 1.6830 - Val Loss: 1.7526\n",
      "Epoch 111 - Train Loss: 1.6858 - Val Loss: 1.7528\n",
      "Epoch 112 - Train Loss: 1.6854 - Val Loss: 1.7531\n",
      "Epoch 113 - Train Loss: 1.6858 - Val Loss: 1.7532\n",
      "Epoch 114 - Train Loss: 1.6930 - Val Loss: 1.7528\n",
      "Epoch 115 - Train Loss: 1.6935 - Val Loss: 1.7524\n",
      "Epoch 116 - Train Loss: 1.6931 - Val Loss: 1.7520\n",
      "Epoch 117 - Train Loss: 1.6845 - Val Loss: 1.7514\n",
      "Epoch 118 - Train Loss: 1.6856 - Val Loss: 1.7515\n",
      "Epoch 119 - Train Loss: 1.6885 - Val Loss: 1.7513\n",
      "Epoch 120 - Train Loss: 1.6882 - Val Loss: 1.7511\n",
      "Epoch 121 - Train Loss: 1.6817 - Val Loss: 1.7511\n",
      "Epoch 122 - Train Loss: 1.6852 - Val Loss: 1.7514\n",
      "Epoch 123 - Train Loss: 1.6858 - Val Loss: 1.7515\n",
      "Epoch 124 - Train Loss: 1.6885 - Val Loss: 1.7515\n",
      "Epoch 125 - Train Loss: 1.6848 - Val Loss: 1.7512\n",
      "Epoch 126 - Train Loss: 1.6883 - Val Loss: 1.7513\n",
      "Epoch 127 - Train Loss: 1.6857 - Val Loss: 1.7514\n",
      "Epoch 128 - Train Loss: 1.6820 - Val Loss: 1.7515\n",
      "Epoch 129 - Train Loss: 1.6927 - Val Loss: 1.7517\n",
      "Epoch 130 - Train Loss: 1.6847 - Val Loss: 1.7514\n",
      "Epoch 131 - Train Loss: 1.6847 - Val Loss: 1.7513\n",
      "Epoch 132 - Train Loss: 1.6861 - Val Loss: 1.7509\n",
      "Epoch 133 - Train Loss: 1.6778 - Val Loss: 1.7503\n",
      "Epoch 134 - Train Loss: 1.6879 - Val Loss: 1.7503\n",
      "Epoch 135 - Train Loss: 1.6893 - Val Loss: 1.7502\n",
      "Epoch 136 - Train Loss: 1.6799 - Val Loss: 1.7503\n",
      "Epoch 137 - Train Loss: 1.6883 - Val Loss: 1.7505\n",
      "Epoch 138 - Train Loss: 1.6873 - Val Loss: 1.7506\n",
      "Epoch 139 - Train Loss: 1.6873 - Val Loss: 1.7503\n",
      "Epoch 140 - Train Loss: 1.6882 - Val Loss: 1.7501\n",
      "Epoch 141 - Train Loss: 1.6856 - Val Loss: 1.7502\n",
      "Epoch 142 - Train Loss: 1.6845 - Val Loss: 1.7500\n",
      "Epoch 143 - Train Loss: 1.6851 - Val Loss: 1.7497\n",
      "Epoch 144 - Train Loss: 1.6886 - Val Loss: 1.7496\n",
      "Epoch 145 - Train Loss: 1.6843 - Val Loss: 1.7490\n",
      "Epoch 146 - Train Loss: 1.6856 - Val Loss: 1.7493\n",
      "Epoch 147 - Train Loss: 1.6844 - Val Loss: 1.7492\n",
      "Epoch 148 - Train Loss: 1.6836 - Val Loss: 1.7492\n",
      "Epoch 149 - Train Loss: 1.6864 - Val Loss: 1.7490\n",
      "Epoch 150 - Train Loss: 1.6846 - Val Loss: 1.7490\n",
      "Epoch 151 - Train Loss: 1.6846 - Val Loss: 1.7490\n",
      "Epoch 152 - Train Loss: 1.6827 - Val Loss: 1.7487\n",
      "Epoch 153 - Train Loss: 1.6883 - Val Loss: 1.7487\n",
      "Epoch 154 - Train Loss: 1.6827 - Val Loss: 1.7487\n",
      "Epoch 155 - Train Loss: 1.6851 - Val Loss: 1.7489\n",
      "Epoch 156 - Train Loss: 1.6872 - Val Loss: 1.7489\n",
      "Epoch 157 - Train Loss: 1.6911 - Val Loss: 1.7487\n",
      "Epoch 158 - Train Loss: 1.6795 - Val Loss: 1.7486\n",
      "Epoch 159 - Train Loss: 1.6848 - Val Loss: 1.7485\n",
      "Epoch 160 - Train Loss: 1.6886 - Val Loss: 1.7484\n",
      "Epoch 161 - Train Loss: 1.6775 - Val Loss: 1.7484\n",
      "Epoch 162 - Train Loss: 1.6827 - Val Loss: 1.7489\n",
      "Epoch 163 - Train Loss: 1.6827 - Val Loss: 1.7489\n",
      "Epoch 164 - Train Loss: 1.6766 - Val Loss: 1.7491\n",
      "Epoch 165 - Train Loss: 1.6851 - Val Loss: 1.7495\n",
      "Epoch 166 - Train Loss: 1.6815 - Val Loss: 1.7493\n",
      "Epoch 167 - Train Loss: 1.6797 - Val Loss: 1.7492\n",
      "Epoch 168 - Train Loss: 1.6815 - Val Loss: 1.7492\n",
      "Epoch 169 - Train Loss: 1.6855 - Val Loss: 1.7492\n",
      "Epoch 170 - Train Loss: 1.6750 - Val Loss: 1.7487\n",
      "Epoch 171 - Train Loss: 1.6802 - Val Loss: 1.7488\n",
      "Epoch 172 - Train Loss: 1.6831 - Val Loss: 1.7488\n",
      "Epoch 173 - Train Loss: 1.6838 - Val Loss: 1.7487\n",
      "Epoch 174 - Train Loss: 1.6783 - Val Loss: 1.7484\n",
      "Epoch 175 - Train Loss: 1.6778 - Val Loss: 1.7485\n",
      "Epoch 176 - Train Loss: 1.6817 - Val Loss: 1.7489\n",
      "Epoch 177 - Train Loss: 1.6805 - Val Loss: 1.7489\n",
      "Epoch 178 - Train Loss: 1.6797 - Val Loss: 1.7485\n",
      "Epoch 179 - Train Loss: 1.6834 - Val Loss: 1.7485\n",
      "Epoch 180 - Train Loss: 1.6787 - Val Loss: 1.7483\n",
      "Epoch 181 - Train Loss: 1.6792 - Val Loss: 1.7483\n",
      "Epoch 182 - Train Loss: 1.6768 - Val Loss: 1.7482\n",
      "Epoch 183 - Train Loss: 1.6829 - Val Loss: 1.7482\n",
      "Epoch 184 - Train Loss: 1.6846 - Val Loss: 1.7482\n",
      "Epoch 185 - Train Loss: 1.6800 - Val Loss: 1.7481\n",
      "Epoch 186 - Train Loss: 1.6805 - Val Loss: 1.7481\n",
      "Epoch 187 - Train Loss: 1.6757 - Val Loss: 1.7483\n",
      "Epoch 188 - Train Loss: 1.6781 - Val Loss: 1.7483\n",
      "Epoch 189 - Train Loss: 1.6829 - Val Loss: 1.7481\n",
      "Epoch 190 - Train Loss: 1.6790 - Val Loss: 1.7478\n",
      "Epoch 191 - Train Loss: 1.6775 - Val Loss: 1.7475\n",
      "Epoch 192 - Train Loss: 1.6734 - Val Loss: 1.7475\n",
      "Epoch 193 - Train Loss: 1.6827 - Val Loss: 1.7475\n",
      "Epoch 194 - Train Loss: 1.6811 - Val Loss: 1.7475\n",
      "Epoch 195 - Train Loss: 1.6824 - Val Loss: 1.7475\n",
      "Epoch 196 - Train Loss: 1.6807 - Val Loss: 1.7474\n",
      "Epoch 197 - Train Loss: 1.6790 - Val Loss: 1.7471\n",
      "Epoch 198 - Train Loss: 1.6863 - Val Loss: 1.7469\n",
      "Epoch 199 - Train Loss: 1.6817 - Val Loss: 1.7466\n",
      "Epoch 200 - Train Loss: 1.6864 - Val Loss: 1.7464\n",
      "Epoch 201 - Train Loss: 1.6780 - Val Loss: 1.7462\n",
      "Epoch 202 - Train Loss: 1.6837 - Val Loss: 1.7465\n",
      "Epoch 203 - Train Loss: 1.6790 - Val Loss: 1.7468\n",
      "Epoch 204 - Train Loss: 1.6811 - Val Loss: 1.7469\n",
      "Epoch 205 - Train Loss: 1.6784 - Val Loss: 1.7469\n",
      "Epoch 206 - Train Loss: 1.6817 - Val Loss: 1.7471\n",
      "Epoch 207 - Train Loss: 1.6801 - Val Loss: 1.7470\n",
      "Epoch 208 - Train Loss: 1.6865 - Val Loss: 1.7468\n",
      "Epoch 209 - Train Loss: 1.6822 - Val Loss: 1.7464\n",
      "Epoch 210 - Train Loss: 1.6792 - Val Loss: 1.7465\n",
      "Epoch 211 - Train Loss: 1.6808 - Val Loss: 1.7465\n",
      "Epoch 212 - Train Loss: 1.6778 - Val Loss: 1.7464\n",
      "Epoch 213 - Train Loss: 1.6800 - Val Loss: 1.7464\n",
      "Epoch 214 - Train Loss: 1.6826 - Val Loss: 1.7465\n",
      "Epoch 215 - Train Loss: 1.6831 - Val Loss: 1.7462\n",
      "Epoch 216 - Train Loss: 1.6784 - Val Loss: 1.7462\n",
      "Epoch 217 - Train Loss: 1.6869 - Val Loss: 1.7463\n",
      "Epoch 218 - Train Loss: 1.6781 - Val Loss: 1.7459\n",
      "Epoch 219 - Train Loss: 1.6800 - Val Loss: 1.7457\n",
      "Epoch 220 - Train Loss: 1.6828 - Val Loss: 1.7458\n",
      "Epoch 221 - Train Loss: 1.6752 - Val Loss: 1.7460\n",
      "Epoch 222 - Train Loss: 1.6832 - Val Loss: 1.7464\n",
      "Epoch 223 - Train Loss: 1.6782 - Val Loss: 1.7463\n",
      "Epoch 224 - Train Loss: 1.6791 - Val Loss: 1.7461\n",
      "Epoch 225 - Train Loss: 1.6823 - Val Loss: 1.7462\n",
      "Epoch 226 - Train Loss: 1.6825 - Val Loss: 1.7460\n",
      "Epoch 227 - Train Loss: 1.6749 - Val Loss: 1.7458\n",
      "Epoch 228 - Train Loss: 1.6788 - Val Loss: 1.7464\n",
      "Epoch 229 - Train Loss: 1.6888 - Val Loss: 1.7461\n",
      "Epoch 230 - Train Loss: 1.6794 - Val Loss: 1.7457\n",
      "Epoch 231 - Train Loss: 1.6840 - Val Loss: 1.7456\n",
      "Epoch 232 - Train Loss: 1.6711 - Val Loss: 1.7451\n",
      "Epoch 233 - Train Loss: 1.6781 - Val Loss: 1.7457\n",
      "Epoch 234 - Train Loss: 1.6792 - Val Loss: 1.7459\n",
      "Epoch 235 - Train Loss: 1.6721 - Val Loss: 1.7457\n",
      "Epoch 236 - Train Loss: 1.6749 - Val Loss: 1.7460\n",
      "Epoch 237 - Train Loss: 1.6783 - Val Loss: 1.7460\n",
      "Epoch 238 - Train Loss: 1.6779 - Val Loss: 1.7457\n",
      "Epoch 239 - Train Loss: 1.6759 - Val Loss: 1.7457\n",
      "Epoch 240 - Train Loss: 1.6805 - Val Loss: 1.7460\n",
      "Epoch 241 - Train Loss: 1.6750 - Val Loss: 1.7460\n",
      "Epoch 242 - Train Loss: 1.6757 - Val Loss: 1.7459\n",
      "Epoch 243 - Train Loss: 1.6800 - Val Loss: 1.7459\n",
      "Epoch 244 - Train Loss: 1.6760 - Val Loss: 1.7459\n",
      "Epoch 245 - Train Loss: 1.6804 - Val Loss: 1.7460\n",
      "Epoch 246 - Train Loss: 1.6847 - Val Loss: 1.7459\n",
      "Epoch 247 - Train Loss: 1.6748 - Val Loss: 1.7451\n",
      "Epoch 248 - Train Loss: 1.6736 - Val Loss: 1.7453\n",
      "Epoch 249 - Train Loss: 1.6784 - Val Loss: 1.7455\n",
      "Epoch 250 - Train Loss: 1.6808 - Val Loss: 1.7458\n",
      "Epoch 251 - Train Loss: 1.6756 - Val Loss: 1.7456\n",
      "Epoch 252 - Train Loss: 1.6742 - Val Loss: 1.7454\n",
      "Epoch 253 - Train Loss: 1.6798 - Val Loss: 1.7454\n",
      "Epoch 254 - Train Loss: 1.6738 - Val Loss: 1.7452\n",
      "Epoch 255 - Train Loss: 1.6773 - Val Loss: 1.7454\n",
      "Epoch 256 - Train Loss: 1.6802 - Val Loss: 1.7453\n",
      "Epoch 257 - Train Loss: 1.6736 - Val Loss: 1.7451\n",
      "Epoch 258 - Train Loss: 1.6755 - Val Loss: 1.7450\n",
      "Epoch 259 - Train Loss: 1.6740 - Val Loss: 1.7454\n",
      "Epoch 260 - Train Loss: 1.6774 - Val Loss: 1.7455\n",
      "Epoch 261 - Train Loss: 1.6787 - Val Loss: 1.7455\n",
      "Epoch 262 - Train Loss: 1.6682 - Val Loss: 1.7458\n",
      "Epoch 263 - Train Loss: 1.6844 - Val Loss: 1.7456\n",
      "Epoch 264 - Train Loss: 1.6724 - Val Loss: 1.7452\n",
      "Epoch 265 - Train Loss: 1.6771 - Val Loss: 1.7451\n",
      "Epoch 266 - Train Loss: 1.6780 - Val Loss: 1.7450\n",
      "Epoch 267 - Train Loss: 1.6812 - Val Loss: 1.7451\n",
      "Epoch 268 - Train Loss: 1.6776 - Val Loss: 1.7449\n",
      "Epoch 269 - Train Loss: 1.6771 - Val Loss: 1.7447\n",
      "Epoch 270 - Train Loss: 1.6749 - Val Loss: 1.7449\n",
      "Epoch 271 - Train Loss: 1.6773 - Val Loss: 1.7452\n",
      "Epoch 272 - Train Loss: 1.6729 - Val Loss: 1.7451\n",
      "Epoch 273 - Train Loss: 1.6723 - Val Loss: 1.7454\n",
      "Epoch 274 - Train Loss: 1.6785 - Val Loss: 1.7455\n",
      "Epoch 275 - Train Loss: 1.6769 - Val Loss: 1.7450\n",
      "Epoch 276 - Train Loss: 1.6765 - Val Loss: 1.7450\n",
      "Epoch 277 - Train Loss: 1.6766 - Val Loss: 1.7448\n",
      "Epoch 278 - Train Loss: 1.6761 - Val Loss: 1.7449\n",
      "Epoch 279 - Train Loss: 1.6799 - Val Loss: 1.7449\n",
      "Epoch 280 - Train Loss: 1.6788 - Val Loss: 1.7446\n",
      "Epoch 281 - Train Loss: 1.6782 - Val Loss: 1.7443\n",
      "Epoch 282 - Train Loss: 1.6766 - Val Loss: 1.7442\n",
      "Epoch 283 - Train Loss: 1.6716 - Val Loss: 1.7447\n",
      "Epoch 284 - Train Loss: 1.6751 - Val Loss: 1.7449\n",
      "Epoch 285 - Train Loss: 1.6741 - Val Loss: 1.7447\n",
      "Epoch 286 - Train Loss: 1.6750 - Val Loss: 1.7449\n",
      "Epoch 287 - Train Loss: 1.6714 - Val Loss: 1.7451\n",
      "Epoch 288 - Train Loss: 1.6777 - Val Loss: 1.7452\n",
      "Epoch 289 - Train Loss: 1.6778 - Val Loss: 1.7449\n",
      "Epoch 290 - Train Loss: 1.6783 - Val Loss: 1.7446\n",
      "Epoch 291 - Train Loss: 1.6782 - Val Loss: 1.7443\n",
      "Epoch 292 - Train Loss: 1.6734 - Val Loss: 1.7439\n",
      "Epoch 293 - Train Loss: 1.6725 - Val Loss: 1.7442\n",
      "Epoch 294 - Train Loss: 1.6721 - Val Loss: 1.7447\n",
      "Epoch 295 - Train Loss: 1.6755 - Val Loss: 1.7448\n",
      "Epoch 296 - Train Loss: 1.6798 - Val Loss: 1.7450\n",
      "Epoch 297 - Train Loss: 1.6753 - Val Loss: 1.7447\n",
      "Epoch 298 - Train Loss: 1.6791 - Val Loss: 1.7445\n",
      "Epoch 299 - Train Loss: 1.6817 - Val Loss: 1.7443\n",
      "Epoch 300 - Train Loss: 1.6800 - Val Loss: 1.7438\n",
      "Epoch 301 - Train Loss: 1.6777 - Val Loss: 1.7436\n",
      "Epoch 302 - Train Loss: 1.6761 - Val Loss: 1.7436\n",
      "Epoch 303 - Train Loss: 1.6760 - Val Loss: 1.7439\n",
      "Epoch 304 - Train Loss: 1.6720 - Val Loss: 1.7440\n",
      "Epoch 305 - Train Loss: 1.6758 - Val Loss: 1.7444\n",
      "Epoch 306 - Train Loss: 1.6751 - Val Loss: 1.7440\n",
      "Epoch 307 - Train Loss: 1.6741 - Val Loss: 1.7439\n",
      "Epoch 308 - Train Loss: 1.6741 - Val Loss: 1.7439\n",
      "Epoch 309 - Train Loss: 1.6766 - Val Loss: 1.7439\n",
      "Epoch 310 - Train Loss: 1.6792 - Val Loss: 1.7440\n",
      "Epoch 311 - Train Loss: 1.6769 - Val Loss: 1.7438\n",
      "Epoch 312 - Train Loss: 1.6746 - Val Loss: 1.7438\n",
      "Epoch 313 - Train Loss: 1.6790 - Val Loss: 1.7440\n",
      "Epoch 314 - Train Loss: 1.6760 - Val Loss: 1.7437\n",
      "Epoch 315 - Train Loss: 1.6764 - Val Loss: 1.7438\n",
      "Epoch 316 - Train Loss: 1.6770 - Val Loss: 1.7441\n",
      "Epoch 317 - Train Loss: 1.6751 - Val Loss: 1.7439\n",
      "Epoch 318 - Train Loss: 1.6749 - Val Loss: 1.7442\n",
      "Epoch 319 - Train Loss: 1.6746 - Val Loss: 1.7439\n",
      "Epoch 320 - Train Loss: 1.6742 - Val Loss: 1.7438\n",
      "Epoch 321 - Train Loss: 1.6768 - Val Loss: 1.7440\n",
      "Epoch 322 - Train Loss: 1.6747 - Val Loss: 1.7439\n",
      "Epoch 323 - Train Loss: 1.6761 - Val Loss: 1.7440\n",
      "Epoch 324 - Train Loss: 1.6783 - Val Loss: 1.7438\n",
      "Epoch 325 - Train Loss: 1.6744 - Val Loss: 1.7434\n",
      "Epoch 326 - Train Loss: 1.6782 - Val Loss: 1.7434\n",
      "Epoch 327 - Train Loss: 1.6750 - Val Loss: 1.7431\n",
      "Epoch 328 - Train Loss: 1.6762 - Val Loss: 1.7432\n",
      "Epoch 329 - Train Loss: 1.6816 - Val Loss: 1.7432\n",
      "Epoch 330 - Train Loss: 1.6726 - Val Loss: 1.7430\n",
      "Epoch 331 - Train Loss: 1.6724 - Val Loss: 1.7432\n",
      "Epoch 332 - Train Loss: 1.6815 - Val Loss: 1.7432\n",
      "Epoch 333 - Train Loss: 1.6705 - Val Loss: 1.7431\n",
      "Epoch 334 - Train Loss: 1.6788 - Val Loss: 1.7432\n",
      "Epoch 335 - Train Loss: 1.6703 - Val Loss: 1.7435\n",
      "Epoch 336 - Train Loss: 1.6748 - Val Loss: 1.7437\n",
      "Epoch 337 - Train Loss: 1.6790 - Val Loss: 1.7438\n",
      "Epoch 338 - Train Loss: 1.6729 - Val Loss: 1.7437\n",
      "Epoch 339 - Train Loss: 1.6745 - Val Loss: 1.7438\n",
      "Epoch 340 - Train Loss: 1.6770 - Val Loss: 1.7438\n",
      "Epoch 341 - Train Loss: 1.6785 - Val Loss: 1.7432\n",
      "Epoch 342 - Train Loss: 1.6668 - Val Loss: 1.7432\n",
      "Epoch 343 - Train Loss: 1.6844 - Val Loss: 1.7435\n",
      "Epoch 344 - Train Loss: 1.6803 - Val Loss: 1.7431\n",
      "Epoch 345 - Train Loss: 1.6699 - Val Loss: 1.7430\n",
      "Epoch 346 - Train Loss: 1.6739 - Val Loss: 1.7432\n",
      "Epoch 347 - Train Loss: 1.6736 - Val Loss: 1.7432\n",
      "Epoch 348 - Train Loss: 1.6749 - Val Loss: 1.7432\n",
      "Epoch 349 - Train Loss: 1.6715 - Val Loss: 1.7432\n",
      "Epoch 350 - Train Loss: 1.6731 - Val Loss: 1.7436\n",
      "Epoch 351 - Train Loss: 1.6715 - Val Loss: 1.7439\n",
      "Epoch 352 - Train Loss: 1.6774 - Val Loss: 1.7437\n",
      "Epoch 353 - Train Loss: 1.6784 - Val Loss: 1.7433\n",
      "Epoch 354 - Train Loss: 1.6756 - Val Loss: 1.7432\n",
      "Epoch 355 - Train Loss: 1.6732 - Val Loss: 1.7432\n",
      "Epoch 356 - Train Loss: 1.6756 - Val Loss: 1.7431\n",
      "Epoch 357 - Train Loss: 1.6720 - Val Loss: 1.7430\n",
      "Epoch 358 - Train Loss: 1.6688 - Val Loss: 1.7432\n",
      "Epoch 359 - Train Loss: 1.6758 - Val Loss: 1.7433\n",
      "Epoch 360 - Train Loss: 1.6829 - Val Loss: 1.7428\n",
      "Epoch 361 - Train Loss: 1.6741 - Val Loss: 1.7424\n",
      "Epoch 362 - Train Loss: 1.6737 - Val Loss: 1.7427\n",
      "Epoch 363 - Train Loss: 1.6756 - Val Loss: 1.7428\n",
      "Epoch 364 - Train Loss: 1.6749 - Val Loss: 1.7430\n",
      "Epoch 365 - Train Loss: 1.6736 - Val Loss: 1.7430\n",
      "Epoch 366 - Train Loss: 1.6759 - Val Loss: 1.7429\n",
      "Epoch 367 - Train Loss: 1.6727 - Val Loss: 1.7429\n",
      "Epoch 368 - Train Loss: 1.6707 - Val Loss: 1.7431\n",
      "Epoch 369 - Train Loss: 1.6729 - Val Loss: 1.7431\n",
      "Epoch 370 - Train Loss: 1.6814 - Val Loss: 1.7427\n",
      "Epoch 371 - Train Loss: 1.6753 - Val Loss: 1.7421\n",
      "Epoch 372 - Train Loss: 1.6798 - Val Loss: 1.7421\n",
      "Epoch 373 - Train Loss: 1.6711 - Val Loss: 1.7420\n",
      "Epoch 374 - Train Loss: 1.6756 - Val Loss: 1.7423\n",
      "Epoch 375 - Train Loss: 1.6754 - Val Loss: 1.7422\n",
      "Epoch 376 - Train Loss: 1.6759 - Val Loss: 1.7421\n",
      "Epoch 377 - Train Loss: 1.6740 - Val Loss: 1.7420\n",
      "Epoch 378 - Train Loss: 1.6746 - Val Loss: 1.7424\n",
      "Epoch 379 - Train Loss: 1.6742 - Val Loss: 1.7427\n",
      "Epoch 380 - Train Loss: 1.6736 - Val Loss: 1.7426\n",
      "Epoch 381 - Train Loss: 1.6667 - Val Loss: 1.7424\n",
      "Epoch 382 - Train Loss: 1.6802 - Val Loss: 1.7427\n",
      "Epoch 383 - Train Loss: 1.6771 - Val Loss: 1.7425\n",
      "Epoch 384 - Train Loss: 1.6716 - Val Loss: 1.7422\n",
      "Epoch 385 - Train Loss: 1.6742 - Val Loss: 1.7422\n",
      "Epoch 386 - Train Loss: 1.6719 - Val Loss: 1.7427\n",
      "Epoch 387 - Train Loss: 1.6811 - Val Loss: 1.7426\n",
      "Epoch 388 - Train Loss: 1.6778 - Val Loss: 1.7424\n",
      "Epoch 389 - Train Loss: 1.6750 - Val Loss: 1.7420\n",
      "Epoch 390 - Train Loss: 1.6721 - Val Loss: 1.7420\n",
      "Epoch 391 - Train Loss: 1.6750 - Val Loss: 1.7424\n",
      "Epoch 392 - Train Loss: 1.6713 - Val Loss: 1.7426\n",
      "Epoch 393 - Train Loss: 1.6708 - Val Loss: 1.7427\n",
      "Epoch 394 - Train Loss: 1.6762 - Val Loss: 1.7428\n",
      "Epoch 395 - Train Loss: 1.6759 - Val Loss: 1.7425\n",
      "Epoch 396 - Train Loss: 1.6743 - Val Loss: 1.7424\n",
      "Epoch 397 - Train Loss: 1.6757 - Val Loss: 1.7420\n",
      "Epoch 398 - Train Loss: 1.6805 - Val Loss: 1.7419\n",
      "Epoch 399 - Train Loss: 1.6783 - Val Loss: 1.7416\n",
      "Epoch 400 - Train Loss: 1.6787 - Val Loss: 1.7413\n",
      "Epoch 401 - Train Loss: 1.6773 - Val Loss: 1.7410\n",
      "Epoch 402 - Train Loss: 1.6712 - Val Loss: 1.7412\n",
      "Epoch 403 - Train Loss: 1.6746 - Val Loss: 1.7416\n",
      "Epoch 404 - Train Loss: 1.6759 - Val Loss: 1.7417\n",
      "Epoch 405 - Train Loss: 1.6741 - Val Loss: 1.7417\n",
      "Epoch 406 - Train Loss: 1.6759 - Val Loss: 1.7418\n",
      "Epoch 407 - Train Loss: 1.6722 - Val Loss: 1.7417\n",
      "Epoch 408 - Train Loss: 1.6743 - Val Loss: 1.7419\n",
      "Epoch 409 - Train Loss: 1.6740 - Val Loss: 1.7418\n",
      "Epoch 410 - Train Loss: 1.6725 - Val Loss: 1.7418\n",
      "Epoch 411 - Train Loss: 1.6754 - Val Loss: 1.7417\n",
      "Epoch 412 - Train Loss: 1.6766 - Val Loss: 1.7415\n",
      "Epoch 413 - Train Loss: 1.6724 - Val Loss: 1.7415\n",
      "Epoch 414 - Train Loss: 1.6764 - Val Loss: 1.7415\n",
      "Epoch 415 - Train Loss: 1.6751 - Val Loss: 1.7412\n",
      "Epoch 416 - Train Loss: 1.6721 - Val Loss: 1.7414\n",
      "Epoch 417 - Train Loss: 1.6726 - Val Loss: 1.7414\n",
      "Epoch 418 - Train Loss: 1.6741 - Val Loss: 1.7416\n",
      "Epoch 419 - Train Loss: 1.6695 - Val Loss: 1.7416\n",
      "Epoch 420 - Train Loss: 1.6704 - Val Loss: 1.7420\n",
      "Epoch 421 - Train Loss: 1.6727 - Val Loss: 1.7424\n",
      "Epoch 422 - Train Loss: 1.6740 - Val Loss: 1.7425\n",
      "Epoch 423 - Train Loss: 1.6697 - Val Loss: 1.7425\n",
      "Epoch 424 - Train Loss: 1.6708 - Val Loss: 1.7425\n",
      "Epoch 425 - Train Loss: 1.6756 - Val Loss: 1.7423\n",
      "Epoch 426 - Train Loss: 1.6732 - Val Loss: 1.7419\n",
      "Epoch 427 - Train Loss: 1.6722 - Val Loss: 1.7418\n",
      "Epoch 428 - Train Loss: 1.6722 - Val Loss: 1.7419\n",
      "Epoch 429 - Train Loss: 1.6712 - Val Loss: 1.7421\n",
      "Epoch 430 - Train Loss: 1.6809 - Val Loss: 1.7422\n",
      "Epoch 431 - Train Loss: 1.6680 - Val Loss: 1.7417\n",
      "Epoch 432 - Train Loss: 1.6787 - Val Loss: 1.7417\n",
      "Epoch 433 - Train Loss: 1.6706 - Val Loss: 1.7417\n",
      "Epoch 434 - Train Loss: 1.6744 - Val Loss: 1.7420\n",
      "Epoch 435 - Train Loss: 1.6683 - Val Loss: 1.7421\n",
      "Epoch 436 - Train Loss: 1.6744 - Val Loss: 1.7420\n",
      "Epoch 437 - Train Loss: 1.6719 - Val Loss: 1.7416\n",
      "Epoch 438 - Train Loss: 1.6682 - Val Loss: 1.7418\n",
      "Epoch 439 - Train Loss: 1.6749 - Val Loss: 1.7418\n",
      "Epoch 440 - Train Loss: 1.6794 - Val Loss: 1.7418\n",
      "Epoch 441 - Train Loss: 1.6754 - Val Loss: 1.7416\n",
      "Epoch 442 - Train Loss: 1.6754 - Val Loss: 1.7413\n",
      "Epoch 443 - Train Loss: 1.6702 - Val Loss: 1.7412\n",
      "Epoch 444 - Train Loss: 1.6749 - Val Loss: 1.7413\n",
      "Epoch 445 - Train Loss: 1.6739 - Val Loss: 1.7412\n",
      "Epoch 446 - Train Loss: 1.6700 - Val Loss: 1.7413\n",
      "Epoch 447 - Train Loss: 1.6765 - Val Loss: 1.7414\n",
      "Epoch 448 - Train Loss: 1.6705 - Val Loss: 1.7412\n",
      "Epoch 449 - Train Loss: 1.6736 - Val Loss: 1.7411\n",
      "Epoch 450 - Train Loss: 1.6800 - Val Loss: 1.7415\n",
      "Epoch 451 - Train Loss: 1.6730 - Val Loss: 1.7414\n",
      "Epoch 452 - Train Loss: 1.6754 - Val Loss: 1.7415\n",
      "Epoch 453 - Train Loss: 1.6711 - Val Loss: 1.7413\n",
      "Epoch 454 - Train Loss: 1.6711 - Val Loss: 1.7412\n",
      "Epoch 455 - Train Loss: 1.6729 - Val Loss: 1.7413\n",
      "Epoch 456 - Train Loss: 1.6762 - Val Loss: 1.7413\n",
      "Epoch 457 - Train Loss: 1.6764 - Val Loss: 1.7410\n",
      "Epoch 458 - Train Loss: 1.6727 - Val Loss: 1.7409\n",
      "Epoch 459 - Train Loss: 1.6738 - Val Loss: 1.7410\n",
      "Epoch 460 - Train Loss: 1.6742 - Val Loss: 1.7410\n",
      "Epoch 461 - Train Loss: 1.6746 - Val Loss: 1.7410\n",
      "Epoch 462 - Train Loss: 1.6691 - Val Loss: 1.7408\n",
      "Epoch 463 - Train Loss: 1.6715 - Val Loss: 1.7410\n",
      "Epoch 464 - Train Loss: 1.6708 - Val Loss: 1.7410\n",
      "Epoch 465 - Train Loss: 1.6757 - Val Loss: 1.7411\n",
      "Epoch 466 - Train Loss: 1.6732 - Val Loss: 1.7411\n",
      "Epoch 467 - Train Loss: 1.6739 - Val Loss: 1.7412\n",
      "Epoch 468 - Train Loss: 1.6733 - Val Loss: 1.7410\n",
      "Epoch 469 - Train Loss: 1.6720 - Val Loss: 1.7408\n",
      "Epoch 470 - Train Loss: 1.6702 - Val Loss: 1.7407\n",
      "Epoch 471 - Train Loss: 1.6744 - Val Loss: 1.7409\n",
      "Epoch 472 - Train Loss: 1.6773 - Val Loss: 1.7408\n",
      "Epoch 473 - Train Loss: 1.6752 - Val Loss: 1.7405\n",
      "Epoch 474 - Train Loss: 1.6709 - Val Loss: 1.7406\n",
      "Epoch 475 - Train Loss: 1.6765 - Val Loss: 1.7405\n",
      "Epoch 476 - Train Loss: 1.6741 - Val Loss: 1.7405\n",
      "Epoch 477 - Train Loss: 1.6745 - Val Loss: 1.7406\n",
      "Epoch 478 - Train Loss: 1.6805 - Val Loss: 1.7406\n",
      "Epoch 479 - Train Loss: 1.6813 - Val Loss: 1.7404\n",
      "Epoch 480 - Train Loss: 1.6684 - Val Loss: 1.7402\n",
      "Epoch 481 - Train Loss: 1.6757 - Val Loss: 1.7403\n",
      "Epoch 482 - Train Loss: 1.6761 - Val Loss: 1.7403\n",
      "Epoch 483 - Train Loss: 1.6738 - Val Loss: 1.7404\n",
      "Epoch 484 - Train Loss: 1.6692 - Val Loss: 1.7407\n",
      "Epoch 485 - Train Loss: 1.6724 - Val Loss: 1.7409\n",
      "Epoch 486 - Train Loss: 1.6705 - Val Loss: 1.7408\n",
      "Epoch 487 - Train Loss: 1.6705 - Val Loss: 1.7407\n",
      "Epoch 488 - Train Loss: 1.6717 - Val Loss: 1.7406\n",
      "Epoch 489 - Train Loss: 1.6684 - Val Loss: 1.7407\n",
      "Epoch 490 - Train Loss: 1.6766 - Val Loss: 1.7408\n",
      "Epoch 491 - Train Loss: 1.6749 - Val Loss: 1.7406\n",
      "Epoch 492 - Train Loss: 1.6698 - Val Loss: 1.7404\n",
      "Epoch 493 - Train Loss: 1.6692 - Val Loss: 1.7406\n",
      "Epoch 494 - Train Loss: 1.6741 - Val Loss: 1.7408\n",
      "Epoch 495 - Train Loss: 1.6701 - Val Loss: 1.7407\n",
      "Epoch 496 - Train Loss: 1.6759 - Val Loss: 1.7409\n",
      "Epoch 497 - Train Loss: 1.6683 - Val Loss: 1.7407\n",
      "Epoch 498 - Train Loss: 1.6733 - Val Loss: 1.7413\n",
      "Epoch 499 - Train Loss: 1.6704 - Val Loss: 1.7413\n",
      "Epoch 500 - Train Loss: 1.6700 - Val Loss: 1.7412\n",
      "Epoch 501 - Train Loss: 1.6760 - Val Loss: 1.7412\n",
      "Epoch 502 - Train Loss: 1.6741 - Val Loss: 1.7408\n",
      "Epoch 503 - Train Loss: 1.6684 - Val Loss: 1.7407\n",
      "Epoch 504 - Train Loss: 1.6705 - Val Loss: 1.7410\n",
      "Epoch 505 - Train Loss: 1.6792 - Val Loss: 1.7409\n",
      "Epoch 506 - Train Loss: 1.6788 - Val Loss: 1.7405\n",
      "Epoch 507 - Train Loss: 1.6753 - Val Loss: 1.7402\n",
      "Epoch 508 - Train Loss: 1.6747 - Val Loss: 1.7402\n",
      "Epoch 509 - Train Loss: 1.6756 - Val Loss: 1.7401\n",
      "Epoch 510 - Train Loss: 1.6733 - Val Loss: 1.7398\n",
      "Epoch 511 - Train Loss: 1.6721 - Val Loss: 1.7399\n",
      "Epoch 512 - Train Loss: 1.6762 - Val Loss: 1.7399\n",
      "Epoch 513 - Train Loss: 1.6742 - Val Loss: 1.7398\n",
      "Epoch 514 - Train Loss: 1.6731 - Val Loss: 1.7400\n",
      "Epoch 515 - Train Loss: 1.6652 - Val Loss: 1.7403\n",
      "Epoch 516 - Train Loss: 1.6710 - Val Loss: 1.7410\n",
      "Epoch 517 - Train Loss: 1.6730 - Val Loss: 1.7411\n",
      "Epoch 518 - Train Loss: 1.6709 - Val Loss: 1.7409\n",
      "Epoch 519 - Train Loss: 1.6789 - Val Loss: 1.7407\n",
      "Epoch 520 - Train Loss: 1.6689 - Val Loss: 1.7403\n",
      "Epoch 521 - Train Loss: 1.6696 - Val Loss: 1.7405\n",
      "Epoch 522 - Train Loss: 1.6722 - Val Loss: 1.7408\n",
      "Epoch 523 - Train Loss: 1.6755 - Val Loss: 1.7407\n",
      "Epoch 524 - Train Loss: 1.6704 - Val Loss: 1.7406\n",
      "Epoch 525 - Train Loss: 1.6700 - Val Loss: 1.7407\n",
      "Epoch 526 - Train Loss: 1.6736 - Val Loss: 1.7410\n",
      "Epoch 527 - Train Loss: 1.6720 - Val Loss: 1.7411\n",
      "Epoch 528 - Train Loss: 1.6667 - Val Loss: 1.7411\n",
      "Epoch 529 - Train Loss: 1.6676 - Val Loss: 1.7412\n",
      "Epoch 530 - Train Loss: 1.6730 - Val Loss: 1.7410\n",
      "Epoch 531 - Train Loss: 1.6668 - Val Loss: 1.7408\n",
      "Epoch 532 - Train Loss: 1.6794 - Val Loss: 1.7408\n",
      "Epoch 533 - Train Loss: 1.6712 - Val Loss: 1.7405\n",
      "Epoch 534 - Train Loss: 1.6709 - Val Loss: 1.7406\n",
      "Epoch 535 - Train Loss: 1.6728 - Val Loss: 1.7407\n",
      "Epoch 536 - Train Loss: 1.6731 - Val Loss: 1.7407\n",
      "Epoch 537 - Train Loss: 1.6795 - Val Loss: 1.7405\n",
      "Epoch 538 - Train Loss: 1.6724 - Val Loss: 1.7402\n",
      "Epoch 539 - Train Loss: 1.6738 - Val Loss: 1.7401\n",
      "Epoch 540 - Train Loss: 1.6700 - Val Loss: 1.7399\n",
      "Epoch 541 - Train Loss: 1.6644 - Val Loss: 1.7403\n",
      "Epoch 542 - Train Loss: 1.6751 - Val Loss: 1.7406\n",
      "Epoch 543 - Train Loss: 1.6740 - Val Loss: 1.7403\n",
      "Epoch 544 - Train Loss: 1.6674 - Val Loss: 1.7404\n",
      "Epoch 545 - Train Loss: 1.6695 - Val Loss: 1.7406\n",
      "Epoch 546 - Train Loss: 1.6691 - Val Loss: 1.7406\n",
      "Epoch 547 - Train Loss: 1.6716 - Val Loss: 1.7407\n",
      "Epoch 548 - Train Loss: 1.6737 - Val Loss: 1.7405\n",
      "Epoch 549 - Train Loss: 1.6700 - Val Loss: 1.7403\n",
      "Epoch 550 - Train Loss: 1.6744 - Val Loss: 1.7403\n",
      "Epoch 551 - Train Loss: 1.6728 - Val Loss: 1.7401\n",
      "Epoch 552 - Train Loss: 1.6710 - Val Loss: 1.7401\n",
      "Epoch 553 - Train Loss: 1.6687 - Val Loss: 1.7400\n",
      "Epoch 554 - Train Loss: 1.6627 - Val Loss: 1.7404\n",
      "Epoch 555 - Train Loss: 1.6644 - Val Loss: 1.7408\n",
      "Epoch 556 - Train Loss: 1.6722 - Val Loss: 1.7412\n",
      "Epoch 557 - Train Loss: 1.6732 - Val Loss: 1.7411\n",
      "Epoch 558 - Train Loss: 1.6727 - Val Loss: 1.7407\n",
      "Epoch 559 - Train Loss: 1.6727 - Val Loss: 1.7405\n",
      "Epoch 560 - Train Loss: 1.6666 - Val Loss: 1.7406\n",
      "Epoch 561 - Train Loss: 1.6707 - Val Loss: 1.7406\n",
      "Epoch 562 - Train Loss: 1.6718 - Val Loss: 1.7406\n",
      "Epoch 563 - Train Loss: 1.6709 - Val Loss: 1.7404\n",
      "Epoch 564 - Train Loss: 1.6724 - Val Loss: 1.7405\n",
      "Epoch 565 - Train Loss: 1.6711 - Val Loss: 1.7402\n",
      "Epoch 566 - Train Loss: 1.6697 - Val Loss: 1.7400\n",
      "Epoch 567 - Train Loss: 1.6668 - Val Loss: 1.7403\n",
      "Epoch 568 - Train Loss: 1.6692 - Val Loss: 1.7407\n",
      "Epoch 569 - Train Loss: 1.6713 - Val Loss: 1.7406\n",
      "Epoch 570 - Train Loss: 1.6702 - Val Loss: 1.7404\n",
      "Epoch 571 - Train Loss: 1.6723 - Val Loss: 1.7406\n",
      "Epoch 572 - Train Loss: 1.6690 - Val Loss: 1.7403\n",
      "Epoch 573 - Train Loss: 1.6666 - Val Loss: 1.7403\n",
      "Epoch 574 - Train Loss: 1.6693 - Val Loss: 1.7406\n",
      "Epoch 575 - Train Loss: 1.6720 - Val Loss: 1.7405\n",
      "Epoch 576 - Train Loss: 1.6736 - Val Loss: 1.7401\n",
      "Epoch 577 - Train Loss: 1.6727 - Val Loss: 1.7403\n",
      "Epoch 578 - Train Loss: 1.6756 - Val Loss: 1.7400\n",
      "Epoch 579 - Train Loss: 1.6665 - Val Loss: 1.7399\n",
      "Epoch 580 - Train Loss: 1.6710 - Val Loss: 1.7400\n",
      "Epoch 581 - Train Loss: 1.6700 - Val Loss: 1.7403\n",
      "Epoch 582 - Train Loss: 1.6714 - Val Loss: 1.7404\n",
      "Epoch 583 - Train Loss: 1.6743 - Val Loss: 1.7403\n",
      "Epoch 584 - Train Loss: 1.6718 - Val Loss: 1.7400\n",
      "Epoch 585 - Train Loss: 1.6742 - Val Loss: 1.7400\n",
      "Epoch 586 - Train Loss: 1.6675 - Val Loss: 1.7399\n",
      "Epoch 587 - Train Loss: 1.6703 - Val Loss: 1.7399\n",
      "Epoch 588 - Train Loss: 1.6729 - Val Loss: 1.7399\n",
      "Epoch 589 - Train Loss: 1.6720 - Val Loss: 1.7397\n",
      "Epoch 590 - Train Loss: 1.6687 - Val Loss: 1.7396\n",
      "Epoch 591 - Train Loss: 1.6795 - Val Loss: 1.7395\n",
      "Epoch 592 - Train Loss: 1.6631 - Val Loss: 1.7395\n",
      "Epoch 593 - Train Loss: 1.6760 - Val Loss: 1.7399\n",
      "Epoch 594 - Train Loss: 1.6721 - Val Loss: 1.7397\n",
      "Epoch 595 - Train Loss: 1.6754 - Val Loss: 1.7395\n",
      "Epoch 596 - Train Loss: 1.6801 - Val Loss: 1.7395\n",
      "Epoch 597 - Train Loss: 1.6735 - Val Loss: 1.7391\n",
      "Epoch 598 - Train Loss: 1.6685 - Val Loss: 1.7389\n",
      "Epoch 599 - Train Loss: 1.6787 - Val Loss: 1.7390\n",
      "Epoch 600 - Train Loss: 1.6727 - Val Loss: 1.7389\n",
      "Epoch 601 - Train Loss: 1.6757 - Val Loss: 1.7393\n",
      "Epoch 602 - Train Loss: 1.6731 - Val Loss: 1.7389\n",
      "Epoch 603 - Train Loss: 1.6722 - Val Loss: 1.7387\n",
      "Epoch 604 - Train Loss: 1.6688 - Val Loss: 1.7387\n",
      "Epoch 605 - Train Loss: 1.6742 - Val Loss: 1.7389\n",
      "Epoch 606 - Train Loss: 1.6690 - Val Loss: 1.7392\n",
      "Epoch 607 - Train Loss: 1.6762 - Val Loss: 1.7394\n",
      "Epoch 608 - Train Loss: 1.6681 - Val Loss: 1.7394\n",
      "Epoch 609 - Train Loss: 1.6684 - Val Loss: 1.7395\n",
      "Epoch 610 - Train Loss: 1.6693 - Val Loss: 1.7396\n",
      "Epoch 611 - Train Loss: 1.6781 - Val Loss: 1.7397\n",
      "Epoch 612 - Train Loss: 1.6699 - Val Loss: 1.7395\n",
      "Epoch 613 - Train Loss: 1.6669 - Val Loss: 1.7396\n",
      "Epoch 614 - Train Loss: 1.6740 - Val Loss: 1.7396\n",
      "Epoch 615 - Train Loss: 1.6713 - Val Loss: 1.7395\n",
      "Epoch 616 - Train Loss: 1.6750 - Val Loss: 1.7395\n",
      "Epoch 617 - Train Loss: 1.6725 - Val Loss: 1.7392\n",
      "Epoch 618 - Train Loss: 1.6745 - Val Loss: 1.7392\n",
      "Epoch 619 - Train Loss: 1.6707 - Val Loss: 1.7392\n",
      "Epoch 620 - Train Loss: 1.6740 - Val Loss: 1.7391\n",
      "Epoch 621 - Train Loss: 1.6665 - Val Loss: 1.7390\n",
      "Epoch 622 - Train Loss: 1.6707 - Val Loss: 1.7395\n",
      "Epoch 623 - Train Loss: 1.6719 - Val Loss: 1.7396\n",
      "Epoch 624 - Train Loss: 1.6685 - Val Loss: 1.7397\n",
      "Epoch 625 - Train Loss: 1.6718 - Val Loss: 1.7396\n",
      "Epoch 626 - Train Loss: 1.6748 - Val Loss: 1.7396\n",
      "Epoch 627 - Train Loss: 1.6653 - Val Loss: 1.7397\n",
      "Epoch 628 - Train Loss: 1.6730 - Val Loss: 1.7397\n",
      "Epoch 629 - Train Loss: 1.6685 - Val Loss: 1.7397\n",
      "Epoch 630 - Train Loss: 1.6721 - Val Loss: 1.7398\n",
      "Epoch 631 - Train Loss: 1.6735 - Val Loss: 1.7395\n",
      "Epoch 632 - Train Loss: 1.6752 - Val Loss: 1.7394\n",
      "Epoch 633 - Train Loss: 1.6738 - Val Loss: 1.7394\n",
      "Epoch 634 - Train Loss: 1.6702 - Val Loss: 1.7392\n",
      "Epoch 635 - Train Loss: 1.6716 - Val Loss: 1.7391\n",
      "Epoch 636 - Train Loss: 1.6746 - Val Loss: 1.7390\n",
      "Epoch 637 - Train Loss: 1.6732 - Val Loss: 1.7393\n",
      "Epoch 638 - Train Loss: 1.6702 - Val Loss: 1.7394\n",
      "Epoch 639 - Train Loss: 1.6704 - Val Loss: 1.7395\n",
      "Epoch 640 - Train Loss: 1.6733 - Val Loss: 1.7395\n",
      "Epoch 641 - Train Loss: 1.6665 - Val Loss: 1.7392\n",
      "Epoch 642 - Train Loss: 1.6800 - Val Loss: 1.7395\n",
      "Epoch 643 - Train Loss: 1.6683 - Val Loss: 1.7391\n",
      "Epoch 644 - Train Loss: 1.6749 - Val Loss: 1.7391\n",
      "Epoch 645 - Train Loss: 1.6788 - Val Loss: 1.7388\n",
      "Epoch 646 - Train Loss: 1.6726 - Val Loss: 1.7383\n",
      "Epoch 647 - Train Loss: 1.6748 - Val Loss: 1.7382\n",
      "Epoch 648 - Train Loss: 1.6726 - Val Loss: 1.7386\n",
      "Epoch 649 - Train Loss: 1.6711 - Val Loss: 1.7386\n",
      "Epoch 650 - Train Loss: 1.6676 - Val Loss: 1.7387\n",
      "Epoch 651 - Train Loss: 1.6660 - Val Loss: 1.7389\n",
      "Epoch 652 - Train Loss: 1.6717 - Val Loss: 1.7390\n",
      "Epoch 653 - Train Loss: 1.6685 - Val Loss: 1.7392\n",
      "Epoch 654 - Train Loss: 1.6757 - Val Loss: 1.7393\n",
      "Epoch 655 - Train Loss: 1.6761 - Val Loss: 1.7391\n",
      "Epoch 656 - Train Loss: 1.6670 - Val Loss: 1.7390\n",
      "Epoch 657 - Train Loss: 1.6685 - Val Loss: 1.7393\n",
      "Epoch 658 - Train Loss: 1.6766 - Val Loss: 1.7393\n",
      "Epoch 659 - Train Loss: 1.6714 - Val Loss: 1.7392\n",
      "Epoch 660 - Train Loss: 1.6638 - Val Loss: 1.7391\n",
      "Epoch 661 - Train Loss: 1.6717 - Val Loss: 1.7394\n",
      "Epoch 662 - Train Loss: 1.6652 - Val Loss: 1.7396\n",
      "Epoch 663 - Train Loss: 1.6741 - Val Loss: 1.7398\n",
      "Epoch 664 - Train Loss: 1.6694 - Val Loss: 1.7398\n",
      "Epoch 665 - Train Loss: 1.6689 - Val Loss: 1.7397\n",
      "Epoch 666 - Train Loss: 1.6741 - Val Loss: 1.7398\n",
      "Epoch 667 - Train Loss: 1.6742 - Val Loss: 1.7393\n",
      "Epoch 668 - Train Loss: 1.6704 - Val Loss: 1.7392\n",
      "Epoch 669 - Train Loss: 1.6742 - Val Loss: 1.7392\n",
      "Epoch 670 - Train Loss: 1.6715 - Val Loss: 1.7390\n",
      "Epoch 671 - Train Loss: 1.6753 - Val Loss: 1.7389\n",
      "Epoch 672 - Train Loss: 1.6694 - Val Loss: 1.7389\n",
      "Epoch 673 - Train Loss: 1.6711 - Val Loss: 1.7391\n",
      "Epoch 674 - Train Loss: 1.6774 - Val Loss: 1.7391\n",
      "Epoch 675 - Train Loss: 1.6765 - Val Loss: 1.7388\n",
      "Epoch 676 - Train Loss: 1.6742 - Val Loss: 1.7386\n",
      "Epoch 677 - Train Loss: 1.6693 - Val Loss: 1.7384\n",
      "Epoch 678 - Train Loss: 1.6771 - Val Loss: 1.7384\n",
      "Epoch 679 - Train Loss: 1.6703 - Val Loss: 1.7383\n",
      "Epoch 680 - Train Loss: 1.6737 - Val Loss: 1.7382\n",
      "Epoch 681 - Train Loss: 1.6666 - Val Loss: 1.7382\n",
      "Epoch 682 - Train Loss: 1.6788 - Val Loss: 1.7382\n",
      "Epoch 683 - Train Loss: 1.6727 - Val Loss: 1.7381\n",
      "Epoch 684 - Train Loss: 1.6771 - Val Loss: 1.7382\n",
      "Epoch 685 - Train Loss: 1.6773 - Val Loss: 1.7380\n",
      "Epoch 686 - Train Loss: 1.6740 - Val Loss: 1.7378\n",
      "Epoch 687 - Train Loss: 1.6668 - Val Loss: 1.7380\n",
      "Epoch 688 - Train Loss: 1.6654 - Val Loss: 1.7385\n",
      "Epoch 689 - Train Loss: 1.6720 - Val Loss: 1.7389\n",
      "Epoch 690 - Train Loss: 1.6699 - Val Loss: 1.7389\n",
      "Epoch 691 - Train Loss: 1.6720 - Val Loss: 1.7388\n",
      "Epoch 692 - Train Loss: 1.6746 - Val Loss: 1.7388\n",
      "Epoch 693 - Train Loss: 1.6732 - Val Loss: 1.7389\n",
      "Epoch 694 - Train Loss: 1.6728 - Val Loss: 1.7389\n",
      "Epoch 695 - Train Loss: 1.6655 - Val Loss: 1.7387\n",
      "Epoch 696 - Train Loss: 1.6685 - Val Loss: 1.7390\n",
      "Epoch 697 - Train Loss: 1.6706 - Val Loss: 1.7391\n",
      "Epoch 698 - Train Loss: 1.6649 - Val Loss: 1.7392\n",
      "Epoch 699 - Train Loss: 1.6712 - Val Loss: 1.7393\n",
      "Epoch 700 - Train Loss: 1.6710 - Val Loss: 1.7393\n",
      "Epoch 701 - Train Loss: 1.6764 - Val Loss: 1.7393\n",
      "Epoch 702 - Train Loss: 1.6693 - Val Loss: 1.7388\n",
      "Epoch 703 - Train Loss: 1.6701 - Val Loss: 1.7386\n",
      "Epoch 704 - Train Loss: 1.6681 - Val Loss: 1.7385\n",
      "Epoch 705 - Train Loss: 1.6676 - Val Loss: 1.7386\n",
      "Epoch 706 - Train Loss: 1.6735 - Val Loss: 1.7386\n",
      "Epoch 707 - Train Loss: 1.6674 - Val Loss: 1.7385\n",
      "Epoch 708 - Train Loss: 1.6704 - Val Loss: 1.7386\n",
      "Epoch 709 - Train Loss: 1.6738 - Val Loss: 1.7385\n",
      "Epoch 710 - Train Loss: 1.6697 - Val Loss: 1.7384\n",
      "Epoch 711 - Train Loss: 1.6693 - Val Loss: 1.7384\n",
      "Epoch 712 - Train Loss: 1.6690 - Val Loss: 1.7384\n",
      "Epoch 713 - Train Loss: 1.6701 - Val Loss: 1.7387\n",
      "Epoch 714 - Train Loss: 1.6703 - Val Loss: 1.7388\n",
      "Epoch 715 - Train Loss: 1.6696 - Val Loss: 1.7387\n",
      "Epoch 716 - Train Loss: 1.6659 - Val Loss: 1.7388\n",
      "Epoch 717 - Train Loss: 1.6714 - Val Loss: 1.7389\n",
      "Epoch 718 - Train Loss: 1.6718 - Val Loss: 1.7392\n",
      "Epoch 719 - Train Loss: 1.6714 - Val Loss: 1.7389\n",
      "Epoch 720 - Train Loss: 1.6729 - Val Loss: 1.7388\n",
      "Epoch 721 - Train Loss: 1.6700 - Val Loss: 1.7385\n",
      "Epoch 722 - Train Loss: 1.6693 - Val Loss: 1.7384\n",
      "Epoch 723 - Train Loss: 1.6700 - Val Loss: 1.7386\n",
      "Epoch 724 - Train Loss: 1.6737 - Val Loss: 1.7388\n",
      "Epoch 725 - Train Loss: 1.6703 - Val Loss: 1.7386\n",
      "Epoch 726 - Train Loss: 1.6697 - Val Loss: 1.7386\n",
      "Epoch 727 - Train Loss: 1.6706 - Val Loss: 1.7387\n",
      "Epoch 728 - Train Loss: 1.6730 - Val Loss: 1.7387\n",
      "Epoch 729 - Train Loss: 1.6705 - Val Loss: 1.7387\n",
      "Epoch 730 - Train Loss: 1.6697 - Val Loss: 1.7388\n",
      "Epoch 731 - Train Loss: 1.6717 - Val Loss: 1.7388\n",
      "Epoch 732 - Train Loss: 1.6646 - Val Loss: 1.7390\n",
      "Epoch 733 - Train Loss: 1.6700 - Val Loss: 1.7390\n",
      "Epoch 734 - Train Loss: 1.6699 - Val Loss: 1.7388\n",
      "Epoch 735 - Train Loss: 1.6704 - Val Loss: 1.7386\n",
      "Epoch 736 - Train Loss: 1.6649 - Val Loss: 1.7387\n",
      "Epoch 737 - Train Loss: 1.6669 - Val Loss: 1.7389\n",
      "Epoch 738 - Train Loss: 1.6733 - Val Loss: 1.7389\n",
      "Epoch 739 - Train Loss: 1.6685 - Val Loss: 1.7388\n",
      "Epoch 740 - Train Loss: 1.6716 - Val Loss: 1.7389\n",
      "Epoch 741 - Train Loss: 1.6729 - Val Loss: 1.7389\n",
      "Epoch 742 - Train Loss: 1.6696 - Val Loss: 1.7387\n",
      "Epoch 743 - Train Loss: 1.6705 - Val Loss: 1.7387\n",
      "Epoch 744 - Train Loss: 1.6724 - Val Loss: 1.7388\n",
      "Epoch 745 - Train Loss: 1.6696 - Val Loss: 1.7389\n",
      "Epoch 746 - Train Loss: 1.6763 - Val Loss: 1.7388\n",
      "Epoch 747 - Train Loss: 1.6684 - Val Loss: 1.7383\n",
      "Epoch 748 - Train Loss: 1.6726 - Val Loss: 1.7381\n",
      "Epoch 749 - Train Loss: 1.6700 - Val Loss: 1.7379\n",
      "Epoch 750 - Train Loss: 1.6737 - Val Loss: 1.7380\n",
      "Epoch 751 - Train Loss: 1.6737 - Val Loss: 1.7380\n",
      "Epoch 752 - Train Loss: 1.6695 - Val Loss: 1.7382\n",
      "Epoch 753 - Train Loss: 1.6739 - Val Loss: 1.7380\n",
      "Epoch 754 - Train Loss: 1.6719 - Val Loss: 1.7380\n",
      "Epoch 755 - Train Loss: 1.6677 - Val Loss: 1.7381\n",
      "Epoch 756 - Train Loss: 1.6714 - Val Loss: 1.7381\n",
      "Epoch 757 - Train Loss: 1.6653 - Val Loss: 1.7381\n",
      "Epoch 758 - Train Loss: 1.6790 - Val Loss: 1.7383\n",
      "Epoch 759 - Train Loss: 1.6715 - Val Loss: 1.7380\n",
      "Epoch 760 - Train Loss: 1.6684 - Val Loss: 1.7380\n",
      "Epoch 761 - Train Loss: 1.6663 - Val Loss: 1.7381\n",
      "Epoch 762 - Train Loss: 1.6716 - Val Loss: 1.7383\n",
      "Epoch 763 - Train Loss: 1.6721 - Val Loss: 1.7383\n",
      "Epoch 764 - Train Loss: 1.6672 - Val Loss: 1.7383\n",
      "Epoch 765 - Train Loss: 1.6721 - Val Loss: 1.7384\n",
      "Epoch 766 - Train Loss: 1.6733 - Val Loss: 1.7382\n",
      "Epoch 767 - Train Loss: 1.6661 - Val Loss: 1.7378\n",
      "Epoch 768 - Train Loss: 1.6697 - Val Loss: 1.7380\n",
      "Epoch 769 - Train Loss: 1.6685 - Val Loss: 1.7380\n",
      "Epoch 770 - Train Loss: 1.6696 - Val Loss: 1.7382\n",
      "Epoch 771 - Train Loss: 1.6687 - Val Loss: 1.7382\n",
      "Epoch 772 - Train Loss: 1.6711 - Val Loss: 1.7383\n",
      "Epoch 773 - Train Loss: 1.6671 - Val Loss: 1.7385\n",
      "Epoch 774 - Train Loss: 1.6739 - Val Loss: 1.7385\n",
      "Epoch 775 - Train Loss: 1.6726 - Val Loss: 1.7385\n",
      "Epoch 776 - Train Loss: 1.6732 - Val Loss: 1.7384\n",
      "Epoch 777 - Train Loss: 1.6702 - Val Loss: 1.7380\n",
      "Epoch 778 - Train Loss: 1.6703 - Val Loss: 1.7381\n",
      "Epoch 779 - Train Loss: 1.6714 - Val Loss: 1.7382\n",
      "Epoch 780 - Train Loss: 1.6710 - Val Loss: 1.7384\n",
      "Epoch 781 - Train Loss: 1.6675 - Val Loss: 1.7381\n",
      "Epoch 782 - Train Loss: 1.6728 - Val Loss: 1.7380\n",
      "Epoch 783 - Train Loss: 1.6697 - Val Loss: 1.7381\n",
      "Epoch 784 - Train Loss: 1.6696 - Val Loss: 1.7383\n",
      "Epoch 785 - Train Loss: 1.6689 - Val Loss: 1.7385\n",
      "Epoch 786 - Train Loss: 1.6671 - Val Loss: 1.7385\n",
      "Epoch 787 - Train Loss: 1.6703 - Val Loss: 1.7384\n",
      "Epoch 788 - Train Loss: 1.6668 - Val Loss: 1.7383\n",
      "Epoch 789 - Train Loss: 1.6694 - Val Loss: 1.7385\n",
      "Epoch 790 - Train Loss: 1.6750 - Val Loss: 1.7382\n",
      "Epoch 791 - Train Loss: 1.6661 - Val Loss: 1.7380\n",
      "Epoch 792 - Train Loss: 1.6672 - Val Loss: 1.7384\n",
      "Epoch 793 - Train Loss: 1.6743 - Val Loss: 1.7387\n",
      "Epoch 794 - Train Loss: 1.6726 - Val Loss: 1.7386\n",
      "Epoch 795 - Train Loss: 1.6738 - Val Loss: 1.7385\n",
      "Epoch 796 - Train Loss: 1.6718 - Val Loss: 1.7387\n",
      "Epoch 797 - Train Loss: 1.6669 - Val Loss: 1.7384\n",
      "Epoch 798 - Train Loss: 1.6630 - Val Loss: 1.7385\n",
      "Epoch 799 - Train Loss: 1.6668 - Val Loss: 1.7390\n",
      "Epoch 800 - Train Loss: 1.6749 - Val Loss: 1.7385\n",
      "Epoch 801 - Train Loss: 1.6633 - Val Loss: 1.7386\n",
      "Epoch 802 - Train Loss: 1.6726 - Val Loss: 1.7386\n",
      "Epoch 803 - Train Loss: 1.6698 - Val Loss: 1.7384\n",
      "Epoch 804 - Train Loss: 1.6773 - Val Loss: 1.7383\n",
      "Epoch 805 - Train Loss: 1.6758 - Val Loss: 1.7379\n",
      "Epoch 806 - Train Loss: 1.6744 - Val Loss: 1.7376\n",
      "Epoch 807 - Train Loss: 1.6686 - Val Loss: 1.7376\n",
      "Epoch 808 - Train Loss: 1.6727 - Val Loss: 1.7377\n",
      "Epoch 809 - Train Loss: 1.6723 - Val Loss: 1.7377\n",
      "Epoch 810 - Train Loss: 1.6765 - Val Loss: 1.7378\n",
      "Epoch 811 - Train Loss: 1.6741 - Val Loss: 1.7376\n",
      "Epoch 812 - Train Loss: 1.6701 - Val Loss: 1.7375\n",
      "Epoch 813 - Train Loss: 1.6708 - Val Loss: 1.7374\n",
      "Epoch 814 - Train Loss: 1.6615 - Val Loss: 1.7374\n",
      "Epoch 815 - Train Loss: 1.6684 - Val Loss: 1.7378\n",
      "Epoch 816 - Train Loss: 1.6688 - Val Loss: 1.7381\n",
      "Epoch 817 - Train Loss: 1.6704 - Val Loss: 1.7384\n",
      "Epoch 818 - Train Loss: 1.6728 - Val Loss: 1.7385\n",
      "Epoch 819 - Train Loss: 1.6708 - Val Loss: 1.7385\n",
      "Epoch 820 - Train Loss: 1.6753 - Val Loss: 1.7379\n",
      "Epoch 821 - Train Loss: 1.6773 - Val Loss: 1.7377\n",
      "Epoch 822 - Train Loss: 1.6693 - Val Loss: 1.7373\n",
      "Epoch 823 - Train Loss: 1.6749 - Val Loss: 1.7371\n",
      "Epoch 824 - Train Loss: 1.6702 - Val Loss: 1.7372\n",
      "Epoch 825 - Train Loss: 1.6736 - Val Loss: 1.7373\n",
      "Epoch 826 - Train Loss: 1.6726 - Val Loss: 1.7371\n",
      "Epoch 827 - Train Loss: 1.6751 - Val Loss: 1.7371\n",
      "Epoch 828 - Train Loss: 1.6712 - Val Loss: 1.7368\n",
      "Epoch 829 - Train Loss: 1.6709 - Val Loss: 1.7369\n",
      "Epoch 830 - Train Loss: 1.6685 - Val Loss: 1.7369\n",
      "Epoch 831 - Train Loss: 1.6665 - Val Loss: 1.7371\n",
      "Epoch 832 - Train Loss: 1.6729 - Val Loss: 1.7375\n",
      "Epoch 833 - Train Loss: 1.6742 - Val Loss: 1.7375\n",
      "Epoch 834 - Train Loss: 1.6733 - Val Loss: 1.7373\n",
      "Epoch 835 - Train Loss: 1.6649 - Val Loss: 1.7375\n",
      "Epoch 836 - Train Loss: 1.6721 - Val Loss: 1.7376\n",
      "Epoch 837 - Train Loss: 1.6736 - Val Loss: 1.7379\n",
      "Epoch 838 - Train Loss: 1.6699 - Val Loss: 1.7377\n",
      "Epoch 839 - Train Loss: 1.6698 - Val Loss: 1.7375\n",
      "Epoch 840 - Train Loss: 1.6753 - Val Loss: 1.7378\n",
      "Epoch 841 - Train Loss: 1.6697 - Val Loss: 1.7377\n",
      "Epoch 842 - Train Loss: 1.6728 - Val Loss: 1.7376\n",
      "Epoch 843 - Train Loss: 1.6755 - Val Loss: 1.7372\n",
      "Epoch 844 - Train Loss: 1.6769 - Val Loss: 1.7369\n",
      "Epoch 845 - Train Loss: 1.6743 - Val Loss: 1.7369\n",
      "Epoch 846 - Train Loss: 1.6672 - Val Loss: 1.7371\n",
      "Epoch 847 - Train Loss: 1.6656 - Val Loss: 1.7375\n",
      "Epoch 848 - Train Loss: 1.6676 - Val Loss: 1.7381\n",
      "Epoch 849 - Train Loss: 1.6759 - Val Loss: 1.7382\n",
      "Epoch 850 - Train Loss: 1.6762 - Val Loss: 1.7378\n",
      "Epoch 851 - Train Loss: 1.6754 - Val Loss: 1.7375\n",
      "Epoch 852 - Train Loss: 1.6747 - Val Loss: 1.7374\n",
      "Epoch 853 - Train Loss: 1.6588 - Val Loss: 1.7375\n",
      "Epoch 854 - Train Loss: 1.6712 - Val Loss: 1.7383\n",
      "Epoch 855 - Train Loss: 1.6719 - Val Loss: 1.7384\n",
      "Epoch 856 - Train Loss: 1.6712 - Val Loss: 1.7381\n",
      "Epoch 857 - Train Loss: 1.6684 - Val Loss: 1.7379\n",
      "Epoch 858 - Train Loss: 1.6652 - Val Loss: 1.7378\n",
      "Epoch 859 - Train Loss: 1.6700 - Val Loss: 1.7380\n",
      "Epoch 860 - Train Loss: 1.6697 - Val Loss: 1.7379\n",
      "Epoch 861 - Train Loss: 1.6765 - Val Loss: 1.7379\n",
      "Epoch 862 - Train Loss: 1.6693 - Val Loss: 1.7375\n",
      "Epoch 863 - Train Loss: 1.6679 - Val Loss: 1.7377\n",
      "Epoch 864 - Train Loss: 1.6666 - Val Loss: 1.7380\n",
      "Epoch 865 - Train Loss: 1.6692 - Val Loss: 1.7381\n",
      "Epoch 866 - Train Loss: 1.6735 - Val Loss: 1.7381\n",
      "Epoch 867 - Train Loss: 1.6654 - Val Loss: 1.7380\n",
      "Epoch 868 - Train Loss: 1.6720 - Val Loss: 1.7381\n",
      "Epoch 869 - Train Loss: 1.6731 - Val Loss: 1.7379\n",
      "Epoch 870 - Train Loss: 1.6638 - Val Loss: 1.7378\n",
      "Epoch 871 - Train Loss: 1.6642 - Val Loss: 1.7380\n",
      "Epoch 872 - Train Loss: 1.6623 - Val Loss: 1.7383\n",
      "Epoch 873 - Train Loss: 1.6718 - Val Loss: 1.7383\n",
      "Epoch 874 - Train Loss: 1.6750 - Val Loss: 1.7380\n",
      "Epoch 875 - Train Loss: 1.6692 - Val Loss: 1.7377\n",
      "Epoch 876 - Train Loss: 1.6708 - Val Loss: 1.7377\n",
      "Epoch 877 - Train Loss: 1.6682 - Val Loss: 1.7378\n",
      "Epoch 878 - Train Loss: 1.6688 - Val Loss: 1.7379\n",
      "Epoch 879 - Train Loss: 1.6688 - Val Loss: 1.7379\n",
      "Epoch 880 - Train Loss: 1.6673 - Val Loss: 1.7379\n",
      "Epoch 881 - Train Loss: 1.6696 - Val Loss: 1.7383\n",
      "Epoch 882 - Train Loss: 1.6670 - Val Loss: 1.7383\n",
      "Epoch 883 - Train Loss: 1.6698 - Val Loss: 1.7383\n",
      "Epoch 884 - Train Loss: 1.6694 - Val Loss: 1.7382\n",
      "Epoch 885 - Train Loss: 1.6708 - Val Loss: 1.7382\n",
      "Epoch 886 - Train Loss: 1.6704 - Val Loss: 1.7380\n",
      "Epoch 887 - Train Loss: 1.6710 - Val Loss: 1.7377\n",
      "Epoch 888 - Train Loss: 1.6681 - Val Loss: 1.7376\n",
      "Epoch 889 - Train Loss: 1.6708 - Val Loss: 1.7376\n",
      "Epoch 890 - Train Loss: 1.6752 - Val Loss: 1.7375\n",
      "Epoch 891 - Train Loss: 1.6683 - Val Loss: 1.7374\n",
      "Epoch 892 - Train Loss: 1.6677 - Val Loss: 1.7375\n",
      "Epoch 893 - Train Loss: 1.6734 - Val Loss: 1.7377\n",
      "Epoch 894 - Train Loss: 1.6753 - Val Loss: 1.7377\n",
      "Epoch 895 - Train Loss: 1.6723 - Val Loss: 1.7374\n",
      "Epoch 896 - Train Loss: 1.6745 - Val Loss: 1.7372\n",
      "Epoch 897 - Train Loss: 1.6737 - Val Loss: 1.7369\n",
      "Epoch 898 - Train Loss: 1.6718 - Val Loss: 1.7372\n",
      "Epoch 899 - Train Loss: 1.6706 - Val Loss: 1.7374\n",
      "Epoch 900 - Train Loss: 1.6667 - Val Loss: 1.7377\n",
      "Epoch 901 - Train Loss: 1.6738 - Val Loss: 1.7378\n",
      "Epoch 902 - Train Loss: 1.6692 - Val Loss: 1.7378\n",
      "Epoch 903 - Train Loss: 1.6782 - Val Loss: 1.7377\n",
      "Epoch 904 - Train Loss: 1.6725 - Val Loss: 1.7374\n",
      "Epoch 905 - Train Loss: 1.6680 - Val Loss: 1.7369\n",
      "Epoch 906 - Train Loss: 1.6669 - Val Loss: 1.7373\n",
      "Epoch 907 - Train Loss: 1.6669 - Val Loss: 1.7378\n",
      "Epoch 908 - Train Loss: 1.6699 - Val Loss: 1.7379\n",
      "Epoch 909 - Train Loss: 1.6712 - Val Loss: 1.7377\n",
      "Epoch 910 - Train Loss: 1.6695 - Val Loss: 1.7376\n",
      "Epoch 911 - Train Loss: 1.6709 - Val Loss: 1.7376\n",
      "Epoch 912 - Train Loss: 1.6670 - Val Loss: 1.7376\n",
      "Epoch 913 - Train Loss: 1.6705 - Val Loss: 1.7378\n",
      "Epoch 914 - Train Loss: 1.6738 - Val Loss: 1.7377\n",
      "Epoch 915 - Train Loss: 1.6680 - Val Loss: 1.7378\n",
      "Epoch 916 - Train Loss: 1.6678 - Val Loss: 1.7379\n",
      "Epoch 917 - Train Loss: 1.6734 - Val Loss: 1.7382\n",
      "Epoch 918 - Train Loss: 1.6657 - Val Loss: 1.7378\n",
      "Epoch 919 - Train Loss: 1.6713 - Val Loss: 1.7380\n",
      "Epoch 920 - Train Loss: 1.6688 - Val Loss: 1.7377\n",
      "Epoch 921 - Train Loss: 1.6645 - Val Loss: 1.7378\n",
      "Epoch 922 - Train Loss: 1.6753 - Val Loss: 1.7376\n",
      "Epoch 923 - Train Loss: 1.6670 - Val Loss: 1.7375\n",
      "Epoch 924 - Train Loss: 1.6639 - Val Loss: 1.7375\n",
      "Epoch 925 - Train Loss: 1.6618 - Val Loss: 1.7378\n",
      "Epoch 926 - Train Loss: 1.6764 - Val Loss: 1.7379\n",
      "Epoch 927 - Train Loss: 1.6685 - Val Loss: 1.7376\n",
      "Epoch 928 - Train Loss: 1.6735 - Val Loss: 1.7377\n",
      "Epoch 929 - Train Loss: 1.6657 - Val Loss: 1.7374\n",
      "Epoch 930 - Train Loss: 1.6672 - Val Loss: 1.7373\n",
      "Epoch 931 - Train Loss: 1.6731 - Val Loss: 1.7373\n",
      "Epoch 932 - Train Loss: 1.6703 - Val Loss: 1.7371\n",
      "Epoch 933 - Train Loss: 1.6705 - Val Loss: 1.7372\n",
      "Epoch 934 - Train Loss: 1.6756 - Val Loss: 1.7371\n",
      "Epoch 935 - Train Loss: 1.6666 - Val Loss: 1.7370\n",
      "Epoch 936 - Train Loss: 1.6678 - Val Loss: 1.7370\n",
      "Epoch 937 - Train Loss: 1.6720 - Val Loss: 1.7370\n",
      "Epoch 938 - Train Loss: 1.6687 - Val Loss: 1.7369\n",
      "Epoch 939 - Train Loss: 1.6715 - Val Loss: 1.7371\n",
      "Epoch 940 - Train Loss: 1.6706 - Val Loss: 1.7371\n",
      "Epoch 941 - Train Loss: 1.6663 - Val Loss: 1.7373\n",
      "Epoch 942 - Train Loss: 1.6724 - Val Loss: 1.7375\n",
      "Epoch 943 - Train Loss: 1.6758 - Val Loss: 1.7373\n",
      "Epoch 944 - Train Loss: 1.6679 - Val Loss: 1.7373\n",
      "Epoch 945 - Train Loss: 1.6687 - Val Loss: 1.7373\n",
      "Epoch 946 - Train Loss: 1.6675 - Val Loss: 1.7372\n",
      "Epoch 947 - Train Loss: 1.6728 - Val Loss: 1.7374\n",
      "Epoch 948 - Train Loss: 1.6716 - Val Loss: 1.7373\n",
      "Epoch 949 - Train Loss: 1.6607 - Val Loss: 1.7372\n",
      "Epoch 950 - Train Loss: 1.6636 - Val Loss: 1.7376\n",
      "Epoch 951 - Train Loss: 1.6675 - Val Loss: 1.7378\n",
      "Epoch 952 - Train Loss: 1.6661 - Val Loss: 1.7377\n",
      "Epoch 953 - Train Loss: 1.6693 - Val Loss: 1.7378\n",
      "Epoch 954 - Train Loss: 1.6725 - Val Loss: 1.7376\n",
      "Epoch 955 - Train Loss: 1.6706 - Val Loss: 1.7375\n",
      "Epoch 956 - Train Loss: 1.6654 - Val Loss: 1.7372\n",
      "Epoch 957 - Train Loss: 1.6645 - Val Loss: 1.7373\n",
      "Epoch 958 - Train Loss: 1.6657 - Val Loss: 1.7376\n",
      "Epoch 959 - Train Loss: 1.6729 - Val Loss: 1.7376\n",
      "Epoch 960 - Train Loss: 1.6729 - Val Loss: 1.7372\n",
      "Epoch 961 - Train Loss: 1.6662 - Val Loss: 1.7372\n",
      "Epoch 962 - Train Loss: 1.6655 - Val Loss: 1.7378\n",
      "Epoch 963 - Train Loss: 1.6651 - Val Loss: 1.7381\n",
      "Epoch 964 - Train Loss: 1.6722 - Val Loss: 1.7381\n",
      "Epoch 965 - Train Loss: 1.6694 - Val Loss: 1.7381\n",
      "Epoch 966 - Train Loss: 1.6696 - Val Loss: 1.7379\n",
      "Epoch 967 - Train Loss: 1.6720 - Val Loss: 1.7376\n",
      "Epoch 968 - Train Loss: 1.6657 - Val Loss: 1.7373\n",
      "Epoch 969 - Train Loss: 1.6696 - Val Loss: 1.7372\n",
      "Epoch 970 - Train Loss: 1.6751 - Val Loss: 1.7373\n",
      "Epoch 971 - Train Loss: 1.6575 - Val Loss: 1.7370\n",
      "Epoch 972 - Train Loss: 1.6715 - Val Loss: 1.7378\n",
      "Epoch 973 - Train Loss: 1.6674 - Val Loss: 1.7378\n",
      "Epoch 974 - Train Loss: 1.6691 - Val Loss: 1.7377\n",
      "Epoch 975 - Train Loss: 1.6649 - Val Loss: 1.7376\n",
      "Epoch 976 - Train Loss: 1.6679 - Val Loss: 1.7379\n",
      "Epoch 977 - Train Loss: 1.6689 - Val Loss: 1.7378\n",
      "Epoch 978 - Train Loss: 1.6689 - Val Loss: 1.7375\n",
      "Epoch 979 - Train Loss: 1.6653 - Val Loss: 1.7375\n",
      "Epoch 980 - Train Loss: 1.6655 - Val Loss: 1.7376\n",
      "Epoch 981 - Train Loss: 1.6664 - Val Loss: 1.7379\n",
      "Epoch 982 - Train Loss: 1.6704 - Val Loss: 1.7380\n",
      "Epoch 983 - Train Loss: 1.6683 - Val Loss: 1.7378\n",
      "Epoch 984 - Train Loss: 1.6639 - Val Loss: 1.7376\n",
      "Epoch 985 - Train Loss: 1.6674 - Val Loss: 1.7377\n",
      "Epoch 986 - Train Loss: 1.6633 - Val Loss: 1.7375\n",
      "Epoch 987 - Train Loss: 1.6729 - Val Loss: 1.7375\n",
      "Epoch 988 - Train Loss: 1.6706 - Val Loss: 1.7374\n",
      "Epoch 989 - Train Loss: 1.6678 - Val Loss: 1.7373\n",
      "Epoch 990 - Train Loss: 1.6610 - Val Loss: 1.7375\n",
      "Epoch 991 - Train Loss: 1.6739 - Val Loss: 1.7378\n",
      "Epoch 992 - Train Loss: 1.6688 - Val Loss: 1.7377\n",
      "Epoch 993 - Train Loss: 1.6755 - Val Loss: 1.7375\n",
      "Epoch 994 - Train Loss: 1.6676 - Val Loss: 1.7372\n",
      "Epoch 995 - Train Loss: 1.6673 - Val Loss: 1.7372\n",
      "Epoch 996 - Train Loss: 1.6690 - Val Loss: 1.7373\n",
      "Epoch 997 - Train Loss: 1.6643 - Val Loss: 1.7372\n",
      "Epoch 998 - Train Loss: 1.6680 - Val Loss: 1.7374\n",
      "Epoch 999 - Train Loss: 1.6665 - Val Loss: 1.7376\n",
      "Epoch 1000 - Train Loss: 1.6718 - Val Loss: 1.7376\n",
      "Epoch 1001 - Train Loss: 1.6750 - Val Loss: 1.7375\n",
      "Epoch 1002 - Train Loss: 1.6720 - Val Loss: 1.7371\n",
      "Epoch 1003 - Train Loss: 1.6641 - Val Loss: 1.7370\n",
      "Epoch 1004 - Train Loss: 1.6665 - Val Loss: 1.7371\n",
      "Epoch 1005 - Train Loss: 1.6635 - Val Loss: 1.7372\n",
      "Epoch 1006 - Train Loss: 1.6677 - Val Loss: 1.7374\n",
      "Epoch 1007 - Train Loss: 1.6756 - Val Loss: 1.7372\n",
      "Epoch 1008 - Train Loss: 1.6706 - Val Loss: 1.7372\n",
      "Epoch 1009 - Train Loss: 1.6644 - Val Loss: 1.7371\n",
      "Epoch 1010 - Train Loss: 1.6776 - Val Loss: 1.7372\n",
      "Epoch 1011 - Train Loss: 1.6669 - Val Loss: 1.7368\n",
      "Epoch 1012 - Train Loss: 1.6714 - Val Loss: 1.7368\n",
      "Epoch 1013 - Train Loss: 1.6639 - Val Loss: 1.7367\n",
      "Epoch 1014 - Train Loss: 1.6685 - Val Loss: 1.7369\n",
      "Epoch 1015 - Train Loss: 1.6668 - Val Loss: 1.7369\n",
      "Epoch 1016 - Train Loss: 1.6671 - Val Loss: 1.7370\n",
      "Epoch 1017 - Train Loss: 1.6683 - Val Loss: 1.7370\n",
      "Epoch 1018 - Train Loss: 1.6664 - Val Loss: 1.7369\n",
      "Epoch 1019 - Train Loss: 1.6689 - Val Loss: 1.7371\n",
      "Epoch 1020 - Train Loss: 1.6699 - Val Loss: 1.7370\n",
      "Epoch 1021 - Train Loss: 1.6703 - Val Loss: 1.7373\n",
      "Epoch 1022 - Train Loss: 1.6708 - Val Loss: 1.7373\n",
      "Epoch 1023 - Train Loss: 1.6678 - Val Loss: 1.7373\n",
      "Epoch 1024 - Train Loss: 1.6672 - Val Loss: 1.7375\n",
      "Epoch 1025 - Train Loss: 1.6656 - Val Loss: 1.7374\n",
      "Epoch 1026 - Train Loss: 1.6664 - Val Loss: 1.7374\n",
      "Epoch 1027 - Train Loss: 1.6673 - Val Loss: 1.7374\n",
      "Epoch 1028 - Train Loss: 1.6636 - Val Loss: 1.7376\n",
      "Epoch 1029 - Train Loss: 1.6663 - Val Loss: 1.7376\n",
      "Epoch 1030 - Train Loss: 1.6676 - Val Loss: 1.7374\n",
      "Epoch 1031 - Train Loss: 1.6661 - Val Loss: 1.7371\n",
      "Epoch 1032 - Train Loss: 1.6730 - Val Loss: 1.7372\n",
      "Epoch 1033 - Train Loss: 1.6771 - Val Loss: 1.7368\n",
      "Epoch 1034 - Train Loss: 1.6724 - Val Loss: 1.7363\n",
      "Epoch 1035 - Train Loss: 1.6747 - Val Loss: 1.7362\n",
      "Epoch 1036 - Train Loss: 1.6727 - Val Loss: 1.7361\n",
      "Epoch 1037 - Train Loss: 1.6638 - Val Loss: 1.7361\n",
      "Epoch 1038 - Train Loss: 1.6642 - Val Loss: 1.7364\n",
      "Epoch 1039 - Train Loss: 1.6759 - Val Loss: 1.7367\n",
      "Epoch 1040 - Train Loss: 1.6710 - Val Loss: 1.7367\n",
      "Epoch 1041 - Train Loss: 1.6743 - Val Loss: 1.7368\n",
      "Epoch 1042 - Train Loss: 1.6719 - Val Loss: 1.7365\n",
      "Epoch 1043 - Train Loss: 1.6708 - Val Loss: 1.7364\n",
      "Epoch 1044 - Train Loss: 1.6668 - Val Loss: 1.7367\n",
      "Epoch 1045 - Train Loss: 1.6712 - Val Loss: 1.7370\n",
      "Epoch 1046 - Train Loss: 1.6707 - Val Loss: 1.7373\n",
      "Epoch 1047 - Train Loss: 1.6766 - Val Loss: 1.7370\n",
      "Epoch 1048 - Train Loss: 1.6644 - Val Loss: 1.7366\n",
      "Epoch 1049 - Train Loss: 1.6647 - Val Loss: 1.7366\n",
      "Epoch 1050 - Train Loss: 1.6669 - Val Loss: 1.7369\n",
      "Epoch 1051 - Train Loss: 1.6661 - Val Loss: 1.7371\n",
      "Epoch 1052 - Train Loss: 1.6731 - Val Loss: 1.7372\n",
      "Epoch 1053 - Train Loss: 1.6686 - Val Loss: 1.7371\n",
      "Epoch 1054 - Train Loss: 1.6668 - Val Loss: 1.7371\n",
      "Epoch 1055 - Train Loss: 1.6633 - Val Loss: 1.7371\n",
      "Epoch 1056 - Train Loss: 1.6737 - Val Loss: 1.7373\n",
      "Epoch 1057 - Train Loss: 1.6633 - Val Loss: 1.7371\n",
      "Epoch 1058 - Train Loss: 1.6700 - Val Loss: 1.7375\n",
      "Epoch 1059 - Train Loss: 1.6706 - Val Loss: 1.7373\n",
      "Epoch 1060 - Train Loss: 1.6727 - Val Loss: 1.7371\n",
      "Epoch 1061 - Train Loss: 1.6618 - Val Loss: 1.7368\n",
      "Epoch 1062 - Train Loss: 1.6694 - Val Loss: 1.7370\n",
      "Epoch 1063 - Train Loss: 1.6685 - Val Loss: 1.7371\n",
      "Epoch 1064 - Train Loss: 1.6657 - Val Loss: 1.7373\n",
      "Epoch 1065 - Train Loss: 1.6710 - Val Loss: 1.7373\n",
      "Epoch 1066 - Train Loss: 1.6744 - Val Loss: 1.7370\n",
      "Epoch 1067 - Train Loss: 1.6690 - Val Loss: 1.7367\n",
      "Epoch 1068 - Train Loss: 1.6655 - Val Loss: 1.7366\n",
      "Epoch 1069 - Train Loss: 1.6705 - Val Loss: 1.7368\n",
      "Epoch 1070 - Train Loss: 1.6704 - Val Loss: 1.7368\n",
      "Epoch 1071 - Train Loss: 1.6639 - Val Loss: 1.7367\n",
      "Epoch 1072 - Train Loss: 1.6697 - Val Loss: 1.7370\n",
      "Epoch 1073 - Train Loss: 1.6664 - Val Loss: 1.7369\n",
      "Epoch 1074 - Train Loss: 1.6714 - Val Loss: 1.7369\n",
      "Epoch 1075 - Train Loss: 1.6625 - Val Loss: 1.7366\n",
      "Epoch 1076 - Train Loss: 1.6685 - Val Loss: 1.7368\n",
      "Epoch 1077 - Train Loss: 1.6708 - Val Loss: 1.7366\n",
      "Epoch 1078 - Train Loss: 1.6705 - Val Loss: 1.7366\n",
      "Epoch 1079 - Train Loss: 1.6643 - Val Loss: 1.7365\n",
      "Epoch 1080 - Train Loss: 1.6630 - Val Loss: 1.7368\n",
      "Epoch 1081 - Train Loss: 1.6634 - Val Loss: 1.7371\n",
      "Epoch 1082 - Train Loss: 1.6704 - Val Loss: 1.7371\n",
      "Epoch 1083 - Train Loss: 1.6680 - Val Loss: 1.7373\n",
      "Epoch 1084 - Train Loss: 1.6729 - Val Loss: 1.7373\n",
      "Epoch 1085 - Train Loss: 1.6692 - Val Loss: 1.7369\n",
      "Epoch 1086 - Train Loss: 1.6646 - Val Loss: 1.7370\n",
      "Epoch 1087 - Train Loss: 1.6704 - Val Loss: 1.7370\n",
      "Epoch 1088 - Train Loss: 1.6702 - Val Loss: 1.7370\n",
      "Epoch 1089 - Train Loss: 1.6706 - Val Loss: 1.7369\n",
      "Epoch 1090 - Train Loss: 1.6724 - Val Loss: 1.7367\n",
      "Epoch 1091 - Train Loss: 1.6732 - Val Loss: 1.7364\n",
      "Epoch 1092 - Train Loss: 1.6729 - Val Loss: 1.7363\n",
      "Epoch 1093 - Train Loss: 1.6678 - Val Loss: 1.7363\n",
      "Epoch 1094 - Train Loss: 1.6716 - Val Loss: 1.7364\n",
      "Epoch 1095 - Train Loss: 1.6679 - Val Loss: 1.7365\n",
      "Epoch 1096 - Train Loss: 1.6777 - Val Loss: 1.7364\n",
      "Epoch 1097 - Train Loss: 1.6714 - Val Loss: 1.7361\n",
      "Epoch 1098 - Train Loss: 1.6628 - Val Loss: 1.7362\n",
      "Epoch 1099 - Train Loss: 1.6634 - Val Loss: 1.7365\n",
      "Epoch 1100 - Train Loss: 1.6720 - Val Loss: 1.7366\n",
      "Epoch 1101 - Train Loss: 1.6703 - Val Loss: 1.7366\n",
      "Epoch 1102 - Train Loss: 1.6691 - Val Loss: 1.7366\n",
      "Epoch 1103 - Train Loss: 1.6748 - Val Loss: 1.7365\n",
      "Epoch 1104 - Train Loss: 1.6688 - Val Loss: 1.7362\n",
      "Epoch 1105 - Train Loss: 1.6682 - Val Loss: 1.7361\n",
      "Epoch 1106 - Train Loss: 1.6666 - Val Loss: 1.7361\n",
      "Epoch 1107 - Train Loss: 1.6706 - Val Loss: 1.7363\n",
      "Epoch 1108 - Train Loss: 1.6681 - Val Loss: 1.7363\n",
      "Epoch 1109 - Train Loss: 1.6669 - Val Loss: 1.7365\n",
      "Epoch 1110 - Train Loss: 1.6621 - Val Loss: 1.7367\n",
      "Epoch 1111 - Train Loss: 1.6641 - Val Loss: 1.7371\n",
      "Epoch 1112 - Train Loss: 1.6654 - Val Loss: 1.7374\n",
      "Epoch 1113 - Train Loss: 1.6701 - Val Loss: 1.7374\n",
      "Epoch 1114 - Train Loss: 1.6685 - Val Loss: 1.7374\n",
      "Epoch 1115 - Train Loss: 1.6641 - Val Loss: 1.7374\n",
      "Epoch 1116 - Train Loss: 1.6685 - Val Loss: 1.7373\n",
      "Epoch 1117 - Train Loss: 1.6695 - Val Loss: 1.7372\n",
      "Epoch 1118 - Train Loss: 1.6677 - Val Loss: 1.7370\n",
      "Epoch 1119 - Train Loss: 1.6661 - Val Loss: 1.7370\n",
      "Epoch 1120 - Train Loss: 1.6662 - Val Loss: 1.7369\n",
      "Epoch 1121 - Train Loss: 1.6685 - Val Loss: 1.7368\n",
      "Epoch 1122 - Train Loss: 1.6744 - Val Loss: 1.7369\n",
      "Epoch 1123 - Train Loss: 1.6702 - Val Loss: 1.7366\n",
      "Epoch 1124 - Train Loss: 1.6669 - Val Loss: 1.7363\n",
      "Epoch 1125 - Train Loss: 1.6666 - Val Loss: 1.7362\n",
      "Epoch 1126 - Train Loss: 1.6711 - Val Loss: 1.7362\n",
      "Epoch 1127 - Train Loss: 1.6698 - Val Loss: 1.7361\n",
      "Epoch 1128 - Train Loss: 1.6695 - Val Loss: 1.7364\n",
      "Epoch 1129 - Train Loss: 1.6681 - Val Loss: 1.7366\n",
      "Epoch 1130 - Train Loss: 1.6688 - Val Loss: 1.7368\n",
      "Epoch 1131 - Train Loss: 1.6696 - Val Loss: 1.7369\n",
      "Epoch 1132 - Train Loss: 1.6715 - Val Loss: 1.7370\n",
      "Epoch 1133 - Train Loss: 1.6656 - Val Loss: 1.7369\n",
      "Epoch 1134 - Train Loss: 1.6702 - Val Loss: 1.7370\n",
      "Epoch 1135 - Train Loss: 1.6695 - Val Loss: 1.7368\n",
      "Epoch 1136 - Train Loss: 1.6784 - Val Loss: 1.7367\n",
      "Epoch 1137 - Train Loss: 1.6667 - Val Loss: 1.7364\n",
      "Epoch 1138 - Train Loss: 1.6698 - Val Loss: 1.7366\n",
      "Epoch 1139 - Train Loss: 1.6665 - Val Loss: 1.7365\n",
      "Epoch 1140 - Train Loss: 1.6731 - Val Loss: 1.7366\n",
      "Epoch 1141 - Train Loss: 1.6715 - Val Loss: 1.7365\n",
      "Epoch 1142 - Train Loss: 1.6670 - Val Loss: 1.7365\n",
      "Epoch 1143 - Train Loss: 1.6733 - Val Loss: 1.7363\n",
      "Epoch 1144 - Train Loss: 1.6735 - Val Loss: 1.7360\n",
      "Epoch 1145 - Train Loss: 1.6702 - Val Loss: 1.7357\n",
      "Epoch 1146 - Train Loss: 1.6711 - Val Loss: 1.7359\n",
      "Epoch 1147 - Train Loss: 1.6655 - Val Loss: 1.7362\n",
      "Epoch 1148 - Train Loss: 1.6665 - Val Loss: 1.7363\n",
      "Epoch 1149 - Train Loss: 1.6685 - Val Loss: 1.7364\n",
      "Epoch 1150 - Train Loss: 1.6725 - Val Loss: 1.7364\n",
      "Epoch 1151 - Train Loss: 1.6731 - Val Loss: 1.7361\n",
      "Epoch 1152 - Train Loss: 1.6683 - Val Loss: 1.7359\n",
      "Epoch 1153 - Train Loss: 1.6696 - Val Loss: 1.7360\n",
      "Epoch 1154 - Train Loss: 1.6642 - Val Loss: 1.7361\n",
      "Epoch 1155 - Train Loss: 1.6642 - Val Loss: 1.7363\n",
      "Epoch 1156 - Train Loss: 1.6736 - Val Loss: 1.7363\n",
      "Epoch 1157 - Train Loss: 1.6702 - Val Loss: 1.7362\n",
      "Epoch 1158 - Train Loss: 1.6667 - Val Loss: 1.7360\n",
      "Epoch 1159 - Train Loss: 1.6662 - Val Loss: 1.7364\n",
      "Epoch 1160 - Train Loss: 1.6750 - Val Loss: 1.7365\n",
      "Epoch 1161 - Train Loss: 1.6657 - Val Loss: 1.7363\n",
      "Epoch 1162 - Train Loss: 1.6639 - Val Loss: 1.7366\n",
      "Epoch 1163 - Train Loss: 1.6650 - Val Loss: 1.7368\n",
      "Epoch 1164 - Train Loss: 1.6692 - Val Loss: 1.7370\n",
      "Epoch 1165 - Train Loss: 1.6666 - Val Loss: 1.7368\n",
      "Epoch 1166 - Train Loss: 1.6672 - Val Loss: 1.7367\n",
      "Epoch 1167 - Train Loss: 1.6710 - Val Loss: 1.7367\n",
      "Epoch 1168 - Train Loss: 1.6699 - Val Loss: 1.7367\n",
      "Epoch 1169 - Train Loss: 1.6692 - Val Loss: 1.7365\n",
      "Epoch 1170 - Train Loss: 1.6667 - Val Loss: 1.7366\n",
      "Epoch 1171 - Train Loss: 1.6688 - Val Loss: 1.7365\n",
      "Epoch 1172 - Train Loss: 1.6698 - Val Loss: 1.7364\n",
      "Epoch 1173 - Train Loss: 1.6666 - Val Loss: 1.7363\n",
      "Epoch 1174 - Train Loss: 1.6745 - Val Loss: 1.7365\n",
      "Epoch 1175 - Train Loss: 1.6652 - Val Loss: 1.7365\n",
      "Epoch 1176 - Train Loss: 1.6688 - Val Loss: 1.7364\n",
      "Epoch 1177 - Train Loss: 1.6700 - Val Loss: 1.7365\n",
      "Epoch 1178 - Train Loss: 1.6716 - Val Loss: 1.7367\n",
      "Epoch 1179 - Train Loss: 1.6655 - Val Loss: 1.7368\n",
      "Epoch 1180 - Train Loss: 1.6677 - Val Loss: 1.7371\n",
      "Epoch 1181 - Train Loss: 1.6745 - Val Loss: 1.7365\n",
      "Epoch 1182 - Train Loss: 1.6686 - Val Loss: 1.7360\n",
      "Epoch 1183 - Train Loss: 1.6612 - Val Loss: 1.7361\n",
      "Epoch 1184 - Train Loss: 1.6710 - Val Loss: 1.7365\n",
      "Epoch 1185 - Train Loss: 1.6707 - Val Loss: 1.7364\n",
      "Epoch 1186 - Train Loss: 1.6690 - Val Loss: 1.7364\n",
      "Epoch 1187 - Train Loss: 1.6760 - Val Loss: 1.7361\n",
      "Epoch 1188 - Train Loss: 1.6721 - Val Loss: 1.7357\n",
      "Epoch 1189 - Train Loss: 1.6710 - Val Loss: 1.7357\n",
      "Epoch 1190 - Train Loss: 1.6603 - Val Loss: 1.7358\n",
      "Epoch 1191 - Train Loss: 1.6670 - Val Loss: 1.7361\n",
      "Epoch 1192 - Train Loss: 1.6671 - Val Loss: 1.7361\n",
      "Epoch 1193 - Train Loss: 1.6646 - Val Loss: 1.7361\n",
      "Epoch 1194 - Train Loss: 1.6702 - Val Loss: 1.7362\n",
      "Epoch 1195 - Train Loss: 1.6658 - Val Loss: 1.7360\n",
      "Epoch 1196 - Train Loss: 1.6692 - Val Loss: 1.7361\n",
      "Epoch 1197 - Train Loss: 1.6634 - Val Loss: 1.7362\n",
      "Epoch 1198 - Train Loss: 1.6767 - Val Loss: 1.7364\n",
      "Epoch 1199 - Train Loss: 1.6681 - Val Loss: 1.7363\n",
      "Epoch 1200 - Train Loss: 1.6680 - Val Loss: 1.7363\n",
      "Epoch 1201 - Train Loss: 1.6681 - Val Loss: 1.7363\n",
      "Epoch 1202 - Train Loss: 1.6698 - Val Loss: 1.7361\n",
      "Epoch 1203 - Train Loss: 1.6717 - Val Loss: 1.7361\n",
      "Epoch 1204 - Train Loss: 1.6683 - Val Loss: 1.7359\n",
      "Epoch 1205 - Train Loss: 1.6713 - Val Loss: 1.7359\n",
      "Epoch 1206 - Train Loss: 1.6591 - Val Loss: 1.7360\n",
      "Epoch 1207 - Train Loss: 1.6665 - Val Loss: 1.7363\n",
      "Epoch 1208 - Train Loss: 1.6686 - Val Loss: 1.7364\n",
      "Epoch 1209 - Train Loss: 1.6687 - Val Loss: 1.7365\n",
      "Epoch 1210 - Train Loss: 1.6647 - Val Loss: 1.7365\n",
      "Epoch 1211 - Train Loss: 1.6650 - Val Loss: 1.7364\n",
      "Epoch 1212 - Train Loss: 1.6718 - Val Loss: 1.7364\n",
      "Epoch 1213 - Train Loss: 1.6680 - Val Loss: 1.7362\n",
      "Epoch 1214 - Train Loss: 1.6703 - Val Loss: 1.7362\n",
      "Epoch 1215 - Train Loss: 1.6723 - Val Loss: 1.7361\n",
      "Epoch 1216 - Train Loss: 1.6660 - Val Loss: 1.7359\n",
      "Epoch 1217 - Train Loss: 1.6718 - Val Loss: 1.7359\n",
      "Epoch 1218 - Train Loss: 1.6679 - Val Loss: 1.7361\n",
      "Epoch 1219 - Train Loss: 1.6739 - Val Loss: 1.7362\n",
      "Epoch 1220 - Train Loss: 1.6685 - Val Loss: 1.7361\n",
      "Epoch 1221 - Train Loss: 1.6697 - Val Loss: 1.7360\n",
      "Epoch 1222 - Train Loss: 1.6674 - Val Loss: 1.7360\n",
      "Epoch 1223 - Train Loss: 1.6682 - Val Loss: 1.7360\n",
      "Epoch 1224 - Train Loss: 1.6755 - Val Loss: 1.7359\n",
      "Epoch 1225 - Train Loss: 1.6662 - Val Loss: 1.7358\n",
      "Epoch 1226 - Train Loss: 1.6703 - Val Loss: 1.7360\n",
      "Epoch 1227 - Train Loss: 1.6682 - Val Loss: 1.7358\n",
      "Epoch 1228 - Train Loss: 1.6624 - Val Loss: 1.7358\n",
      "Epoch 1229 - Train Loss: 1.6640 - Val Loss: 1.7360\n",
      "Epoch 1230 - Train Loss: 1.6732 - Val Loss: 1.7361\n",
      "Epoch 1231 - Train Loss: 1.6698 - Val Loss: 1.7358\n",
      "Epoch 1232 - Train Loss: 1.6653 - Val Loss: 1.7358\n",
      "Epoch 1233 - Train Loss: 1.6658 - Val Loss: 1.7359\n",
      "Epoch 1234 - Train Loss: 1.6667 - Val Loss: 1.7361\n",
      "Epoch 1235 - Train Loss: 1.6707 - Val Loss: 1.7364\n",
      "Epoch 1236 - Train Loss: 1.6684 - Val Loss: 1.7365\n",
      "Epoch 1237 - Train Loss: 1.6698 - Val Loss: 1.7366\n",
      "Epoch 1238 - Train Loss: 1.6643 - Val Loss: 1.7362\n",
      "Epoch 1239 - Train Loss: 1.6673 - Val Loss: 1.7365\n",
      "Epoch 1240 - Train Loss: 1.6746 - Val Loss: 1.7363\n",
      "Epoch 1241 - Train Loss: 1.6703 - Val Loss: 1.7360\n",
      "Epoch 1242 - Train Loss: 1.6687 - Val Loss: 1.7358\n",
      "Epoch 1243 - Train Loss: 1.6724 - Val Loss: 1.7357\n",
      "Epoch 1244 - Train Loss: 1.6708 - Val Loss: 1.7356\n",
      "Epoch 1245 - Train Loss: 1.6717 - Val Loss: 1.7355\n",
      "Epoch 1246 - Train Loss: 1.6656 - Val Loss: 1.7355\n",
      "Epoch 1247 - Train Loss: 1.6660 - Val Loss: 1.7358\n",
      "Epoch 1248 - Train Loss: 1.6677 - Val Loss: 1.7359\n",
      "Epoch 1249 - Train Loss: 1.6611 - Val Loss: 1.7362\n",
      "Epoch 1250 - Train Loss: 1.6732 - Val Loss: 1.7364\n",
      "Epoch 1251 - Train Loss: 1.6678 - Val Loss: 1.7362\n",
      "Epoch 1252 - Train Loss: 1.6592 - Val Loss: 1.7361\n",
      "Epoch 1253 - Train Loss: 1.6626 - Val Loss: 1.7364\n",
      "Epoch 1254 - Train Loss: 1.6657 - Val Loss: 1.7368\n",
      "Epoch 1255 - Train Loss: 1.6672 - Val Loss: 1.7368\n",
      "Epoch 1256 - Train Loss: 1.6656 - Val Loss: 1.7368\n",
      "Epoch 1257 - Train Loss: 1.6672 - Val Loss: 1.7369\n",
      "Epoch 1258 - Train Loss: 1.6743 - Val Loss: 1.7368\n",
      "Epoch 1259 - Train Loss: 1.6713 - Val Loss: 1.7362\n",
      "Epoch 1260 - Train Loss: 1.6689 - Val Loss: 1.7358\n",
      "Epoch 1261 - Train Loss: 1.6801 - Val Loss: 1.7357\n",
      "Epoch 1262 - Train Loss: 1.6734 - Val Loss: 1.7354\n",
      "Epoch 1263 - Train Loss: 1.6659 - Val Loss: 1.7355\n",
      "Epoch 1264 - Train Loss: 1.6694 - Val Loss: 1.7355\n",
      "Epoch 1265 - Train Loss: 1.6663 - Val Loss: 1.7355\n",
      "Epoch 1266 - Train Loss: 1.6704 - Val Loss: 1.7355\n",
      "Epoch 1267 - Train Loss: 1.6702 - Val Loss: 1.7354\n",
      "Epoch 1268 - Train Loss: 1.6678 - Val Loss: 1.7356\n",
      "Epoch 1269 - Train Loss: 1.6662 - Val Loss: 1.7357\n",
      "Epoch 1270 - Train Loss: 1.6685 - Val Loss: 1.7357\n",
      "Epoch 1271 - Train Loss: 1.6653 - Val Loss: 1.7360\n",
      "Epoch 1272 - Train Loss: 1.6700 - Val Loss: 1.7363\n",
      "Epoch 1273 - Train Loss: 1.6686 - Val Loss: 1.7363\n",
      "Epoch 1274 - Train Loss: 1.6667 - Val Loss: 1.7362\n",
      "Epoch 1275 - Train Loss: 1.6754 - Val Loss: 1.7364\n",
      "Epoch 1276 - Train Loss: 1.6686 - Val Loss: 1.7361\n",
      "Epoch 1277 - Train Loss: 1.6659 - Val Loss: 1.7360\n",
      "Epoch 1278 - Train Loss: 1.6694 - Val Loss: 1.7362\n",
      "Epoch 1279 - Train Loss: 1.6713 - Val Loss: 1.7361\n",
      "Epoch 1280 - Train Loss: 1.6650 - Val Loss: 1.7363\n",
      "Epoch 1281 - Train Loss: 1.6713 - Val Loss: 1.7366\n",
      "Epoch 1282 - Train Loss: 1.6622 - Val Loss: 1.7365\n",
      "Epoch 1283 - Train Loss: 1.6621 - Val Loss: 1.7367\n",
      "Epoch 1284 - Train Loss: 1.6682 - Val Loss: 1.7368\n",
      "Epoch 1285 - Train Loss: 1.6653 - Val Loss: 1.7368\n",
      "Epoch 1286 - Train Loss: 1.6696 - Val Loss: 1.7367\n",
      "Epoch 1287 - Train Loss: 1.6673 - Val Loss: 1.7365\n",
      "Epoch 1288 - Train Loss: 1.6641 - Val Loss: 1.7366\n",
      "Epoch 1289 - Train Loss: 1.6657 - Val Loss: 1.7366\n",
      "Epoch 1290 - Train Loss: 1.6619 - Val Loss: 1.7366\n",
      "Epoch 1291 - Train Loss: 1.6631 - Val Loss: 1.7368\n",
      "Epoch 1292 - Train Loss: 1.6696 - Val Loss: 1.7369\n",
      "Epoch 1293 - Train Loss: 1.6685 - Val Loss: 1.7367\n",
      "Epoch 1294 - Train Loss: 1.6682 - Val Loss: 1.7368\n",
      "Epoch 1295 - Train Loss: 1.6684 - Val Loss: 1.7367\n",
      "Epoch 1296 - Train Loss: 1.6665 - Val Loss: 1.7364\n",
      "Epoch 1297 - Train Loss: 1.6747 - Val Loss: 1.7364\n",
      "Epoch 1298 - Train Loss: 1.6682 - Val Loss: 1.7360\n",
      "Epoch 1299 - Train Loss: 1.6713 - Val Loss: 1.7358\n",
      "Epoch 1300 - Train Loss: 1.6677 - Val Loss: 1.7358\n",
      "Epoch 1301 - Train Loss: 1.6744 - Val Loss: 1.7360\n",
      "Epoch 1302 - Train Loss: 1.6659 - Val Loss: 1.7357\n",
      "Epoch 1303 - Train Loss: 1.6676 - Val Loss: 1.7358\n",
      "Epoch 1304 - Train Loss: 1.6642 - Val Loss: 1.7357\n",
      "Epoch 1305 - Train Loss: 1.6653 - Val Loss: 1.7358\n",
      "Epoch 1306 - Train Loss: 1.6700 - Val Loss: 1.7360\n",
      "Epoch 1307 - Train Loss: 1.6723 - Val Loss: 1.7357\n",
      "Epoch 1308 - Train Loss: 1.6646 - Val Loss: 1.7358\n",
      "Epoch 1309 - Train Loss: 1.6695 - Val Loss: 1.7360\n",
      "Epoch 1310 - Train Loss: 1.6710 - Val Loss: 1.7359\n",
      "Epoch 1311 - Train Loss: 1.6664 - Val Loss: 1.7360\n",
      "Epoch 1312 - Train Loss: 1.6647 - Val Loss: 1.7360\n",
      "Epoch 1313 - Train Loss: 1.6652 - Val Loss: 1.7360\n",
      "Epoch 1314 - Train Loss: 1.6724 - Val Loss: 1.7364\n",
      "Epoch 1315 - Train Loss: 1.6711 - Val Loss: 1.7364\n",
      "Epoch 1316 - Train Loss: 1.6675 - Val Loss: 1.7362\n",
      "Epoch 1317 - Train Loss: 1.6684 - Val Loss: 1.7362\n",
      "Epoch 1318 - Train Loss: 1.6684 - Val Loss: 1.7361\n",
      "Epoch 1319 - Train Loss: 1.6708 - Val Loss: 1.7360\n",
      "Epoch 1320 - Train Loss: 1.6627 - Val Loss: 1.7359\n",
      "Epoch 1321 - Train Loss: 1.6659 - Val Loss: 1.7361\n",
      "Epoch 1322 - Train Loss: 1.6682 - Val Loss: 1.7360\n",
      "Epoch 1323 - Train Loss: 1.6650 - Val Loss: 1.7361\n",
      "Epoch 1324 - Train Loss: 1.6651 - Val Loss: 1.7363\n",
      "Epoch 1325 - Train Loss: 1.6668 - Val Loss: 1.7365\n",
      "Epoch 1326 - Train Loss: 1.6613 - Val Loss: 1.7364\n",
      "Epoch 1327 - Train Loss: 1.6708 - Val Loss: 1.7363\n",
      "Epoch 1328 - Train Loss: 1.6662 - Val Loss: 1.7361\n",
      "Epoch 1329 - Train Loss: 1.6675 - Val Loss: 1.7360\n",
      "Epoch 1330 - Train Loss: 1.6678 - Val Loss: 1.7358\n",
      "Epoch 1331 - Train Loss: 1.6717 - Val Loss: 1.7356\n",
      "Epoch 1332 - Train Loss: 1.6676 - Val Loss: 1.7356\n",
      "Epoch 1333 - Train Loss: 1.6678 - Val Loss: 1.7357\n",
      "Epoch 1334 - Train Loss: 1.6669 - Val Loss: 1.7357\n",
      "Epoch 1335 - Train Loss: 1.6690 - Val Loss: 1.7358\n",
      "Epoch 1336 - Train Loss: 1.6711 - Val Loss: 1.7357\n",
      "Epoch 1337 - Train Loss: 1.6680 - Val Loss: 1.7357\n",
      "Epoch 1338 - Train Loss: 1.6601 - Val Loss: 1.7357\n",
      "Epoch 1339 - Train Loss: 1.6712 - Val Loss: 1.7360\n",
      "Epoch 1340 - Train Loss: 1.6691 - Val Loss: 1.7361\n",
      "Epoch 1341 - Train Loss: 1.6645 - Val Loss: 1.7362\n",
      "Epoch 1342 - Train Loss: 1.6735 - Val Loss: 1.7363\n",
      "Epoch 1343 - Train Loss: 1.6671 - Val Loss: 1.7361\n",
      "Epoch 1344 - Train Loss: 1.6677 - Val Loss: 1.7361\n",
      "Epoch 1345 - Train Loss: 1.6637 - Val Loss: 1.7362\n",
      "Epoch 1346 - Train Loss: 1.6601 - Val Loss: 1.7361\n",
      "Epoch 1347 - Train Loss: 1.6665 - Val Loss: 1.7364\n",
      "Epoch 1348 - Train Loss: 1.6736 - Val Loss: 1.7364\n",
      "Epoch 1349 - Train Loss: 1.6681 - Val Loss: 1.7362\n",
      "Epoch 1350 - Train Loss: 1.6699 - Val Loss: 1.7361\n",
      "Epoch 1351 - Train Loss: 1.6627 - Val Loss: 1.7358\n",
      "Epoch 1352 - Train Loss: 1.6646 - Val Loss: 1.7359\n",
      "Epoch 1353 - Train Loss: 1.6682 - Val Loss: 1.7359\n",
      "Epoch 1354 - Train Loss: 1.6696 - Val Loss: 1.7359\n",
      "Epoch 1355 - Train Loss: 1.6727 - Val Loss: 1.7357\n",
      "Epoch 1356 - Train Loss: 1.6693 - Val Loss: 1.7352\n",
      "Epoch 1357 - Train Loss: 1.6721 - Val Loss: 1.7352\n",
      "Epoch 1358 - Train Loss: 1.6713 - Val Loss: 1.7353\n",
      "Epoch 1359 - Train Loss: 1.6644 - Val Loss: 1.7355\n",
      "Epoch 1360 - Train Loss: 1.6687 - Val Loss: 1.7359\n",
      "Epoch 1361 - Train Loss: 1.6619 - Val Loss: 1.7362\n",
      "Epoch 1362 - Train Loss: 1.6688 - Val Loss: 1.7363\n",
      "Epoch 1363 - Train Loss: 1.6700 - Val Loss: 1.7363\n",
      "Epoch 1364 - Train Loss: 1.6796 - Val Loss: 1.7360\n",
      "Epoch 1365 - Train Loss: 1.6674 - Val Loss: 1.7355\n",
      "Epoch 1366 - Train Loss: 1.6596 - Val Loss: 1.7355\n",
      "Epoch 1367 - Train Loss: 1.6716 - Val Loss: 1.7358\n",
      "Epoch 1368 - Train Loss: 1.6710 - Val Loss: 1.7357\n",
      "Epoch 1369 - Train Loss: 1.6686 - Val Loss: 1.7355\n",
      "Epoch 1370 - Train Loss: 1.6712 - Val Loss: 1.7357\n",
      "Epoch 1371 - Train Loss: 1.6633 - Val Loss: 1.7356\n",
      "Epoch 1372 - Train Loss: 1.6722 - Val Loss: 1.7357\n",
      "Epoch 1373 - Train Loss: 1.6753 - Val Loss: 1.7356\n",
      "Epoch 1374 - Train Loss: 1.6610 - Val Loss: 1.7358\n",
      "Epoch 1375 - Train Loss: 1.6675 - Val Loss: 1.7364\n",
      "Epoch 1376 - Train Loss: 1.6643 - Val Loss: 1.7365\n",
      "Epoch 1377 - Train Loss: 1.6651 - Val Loss: 1.7367\n",
      "Epoch 1378 - Train Loss: 1.6668 - Val Loss: 1.7369\n",
      "Epoch 1379 - Train Loss: 1.6657 - Val Loss: 1.7371\n",
      "Epoch 1380 - Train Loss: 1.6656 - Val Loss: 1.7368\n",
      "Epoch 1381 - Train Loss: 1.6659 - Val Loss: 1.7365\n",
      "Epoch 1382 - Train Loss: 1.6664 - Val Loss: 1.7362\n",
      "Epoch 1383 - Train Loss: 1.6713 - Val Loss: 1.7364\n",
      "Epoch 1384 - Train Loss: 1.6677 - Val Loss: 1.7363\n",
      "Epoch 1385 - Train Loss: 1.6678 - Val Loss: 1.7362\n",
      "Epoch 1386 - Train Loss: 1.6663 - Val Loss: 1.7360\n",
      "Epoch 1387 - Train Loss: 1.6713 - Val Loss: 1.7361\n",
      "Epoch 1388 - Train Loss: 1.6637 - Val Loss: 1.7362\n",
      "Epoch 1389 - Train Loss: 1.6716 - Val Loss: 1.7361\n",
      "Epoch 1390 - Train Loss: 1.6712 - Val Loss: 1.7360\n",
      "Epoch 1391 - Train Loss: 1.6648 - Val Loss: 1.7360\n",
      "Epoch 1392 - Train Loss: 1.6666 - Val Loss: 1.7361\n",
      "Epoch 1393 - Train Loss: 1.6732 - Val Loss: 1.7362\n",
      "Epoch 1394 - Train Loss: 1.6688 - Val Loss: 1.7358\n",
      "Epoch 1395 - Train Loss: 1.6721 - Val Loss: 1.7358\n",
      "Epoch 1396 - Train Loss: 1.6636 - Val Loss: 1.7357\n",
      "Epoch 1397 - Train Loss: 1.6688 - Val Loss: 1.7360\n",
      "Epoch 1398 - Train Loss: 1.6698 - Val Loss: 1.7358\n",
      "Epoch 1399 - Train Loss: 1.6646 - Val Loss: 1.7356\n",
      "Epoch 1400 - Train Loss: 1.6719 - Val Loss: 1.7356\n",
      "Epoch 1401 - Train Loss: 1.6653 - Val Loss: 1.7354\n",
      "Epoch 1402 - Train Loss: 1.6660 - Val Loss: 1.7354\n",
      "Epoch 1403 - Train Loss: 1.6664 - Val Loss: 1.7357\n",
      "Epoch 1404 - Train Loss: 1.6739 - Val Loss: 1.7359\n",
      "Epoch 1405 - Train Loss: 1.6683 - Val Loss: 1.7356\n",
      "Epoch 1406 - Train Loss: 1.6717 - Val Loss: 1.7357\n",
      "Epoch 1407 - Train Loss: 1.6631 - Val Loss: 1.7355\n",
      "Epoch 1408 - Train Loss: 1.6702 - Val Loss: 1.7358\n",
      "Epoch 1409 - Train Loss: 1.6693 - Val Loss: 1.7357\n",
      "Epoch 1410 - Train Loss: 1.6736 - Val Loss: 1.7354\n",
      "Epoch 1411 - Train Loss: 1.6700 - Val Loss: 1.7354\n",
      "Epoch 1412 - Train Loss: 1.6663 - Val Loss: 1.7354\n",
      "Epoch 1413 - Train Loss: 1.6749 - Val Loss: 1.7356\n",
      "Epoch 1414 - Train Loss: 1.6663 - Val Loss: 1.7354\n",
      "Epoch 1415 - Train Loss: 1.6658 - Val Loss: 1.7354\n",
      "Epoch 1416 - Train Loss: 1.6681 - Val Loss: 1.7354\n",
      "Epoch 1417 - Train Loss: 1.6637 - Val Loss: 1.7353\n",
      "Epoch 1418 - Train Loss: 1.6663 - Val Loss: 1.7357\n",
      "Epoch 1419 - Train Loss: 1.6675 - Val Loss: 1.7360\n",
      "Epoch 1420 - Train Loss: 1.6711 - Val Loss: 1.7361\n",
      "Epoch 1421 - Train Loss: 1.6698 - Val Loss: 1.7358\n",
      "Epoch 1422 - Train Loss: 1.6739 - Val Loss: 1.7356\n",
      "Epoch 1423 - Train Loss: 1.6658 - Val Loss: 1.7353\n",
      "Epoch 1424 - Train Loss: 1.6703 - Val Loss: 1.7352\n",
      "Epoch 1425 - Train Loss: 1.6697 - Val Loss: 1.7349\n",
      "Epoch 1426 - Train Loss: 1.6703 - Val Loss: 1.7351\n",
      "Epoch 1427 - Train Loss: 1.6706 - Val Loss: 1.7353\n",
      "Epoch 1428 - Train Loss: 1.6684 - Val Loss: 1.7353\n",
      "Epoch 1429 - Train Loss: 1.6658 - Val Loss: 1.7354\n",
      "Epoch 1430 - Train Loss: 1.6723 - Val Loss: 1.7356\n",
      "Epoch 1431 - Train Loss: 1.6713 - Val Loss: 1.7355\n",
      "Epoch 1432 - Train Loss: 1.6663 - Val Loss: 1.7356\n",
      "Epoch 1433 - Train Loss: 1.6622 - Val Loss: 1.7355\n",
      "Epoch 1434 - Train Loss: 1.6645 - Val Loss: 1.7357\n",
      "Epoch 1435 - Train Loss: 1.6666 - Val Loss: 1.7359\n",
      "Epoch 1436 - Train Loss: 1.6675 - Val Loss: 1.7359\n",
      "Epoch 1437 - Train Loss: 1.6644 - Val Loss: 1.7358\n",
      "Epoch 1438 - Train Loss: 1.6616 - Val Loss: 1.7360\n",
      "Epoch 1439 - Train Loss: 1.6671 - Val Loss: 1.7363\n",
      "Epoch 1440 - Train Loss: 1.6730 - Val Loss: 1.7361\n",
      "Epoch 1441 - Train Loss: 1.6702 - Val Loss: 1.7357\n",
      "Epoch 1442 - Train Loss: 1.6609 - Val Loss: 1.7355\n",
      "Epoch 1443 - Train Loss: 1.6718 - Val Loss: 1.7357\n",
      "Epoch 1444 - Train Loss: 1.6734 - Val Loss: 1.7354\n",
      "Epoch 1445 - Train Loss: 1.6684 - Val Loss: 1.7351\n",
      "Epoch 1446 - Train Loss: 1.6598 - Val Loss: 1.7352\n",
      "Epoch 1447 - Train Loss: 1.6666 - Val Loss: 1.7355\n",
      "Epoch 1448 - Train Loss: 1.6661 - Val Loss: 1.7357\n",
      "Epoch 1449 - Train Loss: 1.6671 - Val Loss: 1.7359\n",
      "Epoch 1450 - Train Loss: 1.6613 - Val Loss: 1.7361\n",
      "Epoch 1451 - Train Loss: 1.6682 - Val Loss: 1.7362\n",
      "Epoch 1452 - Train Loss: 1.6671 - Val Loss: 1.7360\n",
      "Epoch 1453 - Train Loss: 1.6651 - Val Loss: 1.7357\n",
      "Epoch 1454 - Train Loss: 1.6632 - Val Loss: 1.7357\n",
      "Epoch 1455 - Train Loss: 1.6656 - Val Loss: 1.7359\n",
      "Epoch 1456 - Train Loss: 1.6680 - Val Loss: 1.7360\n",
      "Epoch 1457 - Train Loss: 1.6755 - Val Loss: 1.7358\n",
      "Epoch 1458 - Train Loss: 1.6717 - Val Loss: 1.7354\n",
      "Epoch 1459 - Train Loss: 1.6657 - Val Loss: 1.7352\n",
      "Epoch 1460 - Train Loss: 1.6736 - Val Loss: 1.7350\n",
      "Epoch 1461 - Train Loss: 1.6708 - Val Loss: 1.7350\n",
      "Epoch 1462 - Train Loss: 1.6611 - Val Loss: 1.7350\n",
      "Epoch 1463 - Train Loss: 1.6619 - Val Loss: 1.7353\n",
      "Epoch 1464 - Train Loss: 1.6708 - Val Loss: 1.7354\n",
      "Epoch 1465 - Train Loss: 1.6735 - Val Loss: 1.7354\n",
      "Epoch 1466 - Train Loss: 1.6707 - Val Loss: 1.7353\n",
      "Epoch 1467 - Train Loss: 1.6660 - Val Loss: 1.7352\n",
      "Epoch 1468 - Train Loss: 1.6622 - Val Loss: 1.7352\n",
      "Epoch 1469 - Train Loss: 1.6679 - Val Loss: 1.7355\n",
      "Epoch 1470 - Train Loss: 1.6615 - Val Loss: 1.7355\n",
      "Epoch 1471 - Train Loss: 1.6664 - Val Loss: 1.7359\n",
      "Epoch 1472 - Train Loss: 1.6692 - Val Loss: 1.7360\n",
      "Epoch 1473 - Train Loss: 1.6694 - Val Loss: 1.7358\n",
      "Epoch 1474 - Train Loss: 1.6683 - Val Loss: 1.7355\n",
      "Epoch 1475 - Train Loss: 1.6642 - Val Loss: 1.7357\n",
      "Epoch 1476 - Train Loss: 1.6705 - Val Loss: 1.7359\n",
      "Epoch 1477 - Train Loss: 1.6719 - Val Loss: 1.7357\n",
      "Epoch 1478 - Train Loss: 1.6691 - Val Loss: 1.7355\n",
      "Epoch 1479 - Train Loss: 1.6633 - Val Loss: 1.7355\n",
      "Epoch 1480 - Train Loss: 1.6669 - Val Loss: 1.7357\n",
      "Epoch 1481 - Train Loss: 1.6635 - Val Loss: 1.7357\n",
      "Epoch 1482 - Train Loss: 1.6729 - Val Loss: 1.7359\n",
      "Epoch 1483 - Train Loss: 1.6704 - Val Loss: 1.7356\n",
      "Epoch 1484 - Train Loss: 1.6690 - Val Loss: 1.7354\n",
      "Epoch 1485 - Train Loss: 1.6654 - Val Loss: 1.7352\n",
      "Epoch 1486 - Train Loss: 1.6736 - Val Loss: 1.7352\n",
      "Epoch 1487 - Train Loss: 1.6627 - Val Loss: 1.7351\n",
      "Epoch 1488 - Train Loss: 1.6632 - Val Loss: 1.7353\n",
      "Epoch 1489 - Train Loss: 1.6704 - Val Loss: 1.7355\n",
      "Epoch 1490 - Train Loss: 1.6631 - Val Loss: 1.7354\n",
      "Epoch 1491 - Train Loss: 1.6635 - Val Loss: 1.7357\n",
      "Epoch 1492 - Train Loss: 1.6686 - Val Loss: 1.7359\n",
      "Epoch 1493 - Train Loss: 1.6673 - Val Loss: 1.7358\n",
      "Epoch 1494 - Train Loss: 1.6722 - Val Loss: 1.7357\n",
      "Epoch 1495 - Train Loss: 1.6721 - Val Loss: 1.7354\n",
      "Epoch 1496 - Train Loss: 1.6774 - Val Loss: 1.7352\n",
      "Epoch 1497 - Train Loss: 1.6671 - Val Loss: 1.7347\n",
      "Epoch 1498 - Train Loss: 1.6706 - Val Loss: 1.7347\n",
      "Epoch 1499 - Train Loss: 1.6646 - Val Loss: 1.7348\n",
      "Epoch 1500 - Train Loss: 1.6691 - Val Loss: 1.7350\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(cfg.EPOCH):\n",
    "    model.train()\n",
    "    loss_ = 0\n",
    "\n",
    "    for x_, y_ in train_loader:\n",
    "        x_ = x_.to('cuda', non_blocking=True)\n",
    "        y_ = y_.to('cuda', non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_)\n",
    "        loss = torch.sqrt(criterion(outputs, y_))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_ += loss.item()\n",
    "\n",
    "    train_losses.append(loss_ / len(train_loader))\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_, y_ in val_loader:  \n",
    "            x_ = x_.to('cuda', non_blocking=True)\n",
    "            y_ = y_.to('cuda', non_blocking=True)\n",
    "            val_output = model(x_)\n",
    "            val_loss += torch.sqrt(criterion(val_output, y_)).item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    # if epoch+1 % 20 == 0:\n",
    "    print(f'Epoch {epoch+1} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6108722d-5762-485d-babe-19701c2ff007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Val')\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel('Train- Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520b35d-cf7e-4597-acc6-b43dd97edcdb",
   "metadata": {},
   "source": [
    "> Without `skills` model is performing good\n",
    "\n",
    "The reason for this NN behavoiur is simple:\n",
    "\n",
    "0. Might be skill issue\n",
    "1. The dataset is small\n",
    "2. The dataset is agumented, mean random values or users are added in it and there rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b43def-79f0-4318-a6e4-215afc6da2a6",
   "metadata": {},
   "source": [
    "will be using only these model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
