{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1544cb55-4361-4e37-b775-3a6f4cdf564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from score import score\n",
    "from rec_utils import split_the_data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from surprise import SVDpp, BaselineOnly\n",
    "from surprise.model_selection import train_test_split as sur_tts\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da859d77-87e0-4fc0-9b39-f8e5f3a495f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(\"../Course_Scraper/assets/augumented_data/augmented_user_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b03599-5f27-4200-8d9c-c9c84643d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "RMSE: 1.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6403549357897722"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(user_data, reader)\n",
    "train, test = sur_tts(data, test_size=0.2, shuffle=True)\n",
    "\n",
    "bsl_options = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"method\": \"sgd\",\n",
    "    \"learning_rate\": 0.001,\n",
    "}\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "algo.fit(train)\n",
    "predictions = algo.test(test)\n",
    "accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "918567ae-6542-49b7-9dc1-33d5f2df7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, test = split_the_data(user_data)\n",
    "train, val = split_the_data(data)\n",
    "\n",
    "X_train = train.drop('rating', axis=1)\n",
    "X_val = val.drop('rating', axis=1)\n",
    "\n",
    "y_train = train['rating']\n",
    "y_val = val['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c8954a9-687d-4def-82ee-37882cd93e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys(nn.Module):\n",
    "    def __init__(self, input_: int = 2, output: int = 1):\n",
    "        super(RecSys, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4a8d2ee-00bc-4ff4-a75e-a8d9439be2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecSys()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15e8fd37-1e93-4617-828e-e112e0d60165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 4.316611289978027 - Val Loss 5.055158615112305\n",
      "Epoch 2 - Train Loss: 4.251636028289795 - Val Loss 4.370450496673584\n",
      "Epoch 3 - Train Loss: 4.203991413116455 - Val Loss 4.071814060211182\n",
      "Epoch 4 - Train Loss: 4.167439937591553 - Val Loss 3.916330099105835\n",
      "Epoch 5 - Train Loss: 4.131375789642334 - Val Loss 3.8279974460601807\n",
      "Epoch 6 - Train Loss: 4.097622394561768 - Val Loss 3.77929949760437\n",
      "Epoch 7 - Train Loss: 4.063751220703125 - Val Loss 3.747234582901001\n",
      "Epoch 8 - Train Loss: 4.028741359710693 - Val Loss 3.720527172088623\n",
      "Epoch 9 - Train Loss: 3.993804454803467 - Val Loss 3.6946182250976562\n",
      "Epoch 10 - Train Loss: 3.960301637649536 - Val Loss 3.6663002967834473\n",
      "Epoch 11 - Train Loss: 3.928234577178955 - Val Loss 3.6396963596343994\n",
      "Epoch 12 - Train Loss: 3.8985655307769775 - Val Loss 3.614410161972046\n",
      "Epoch 13 - Train Loss: 3.8681070804595947 - Val Loss 3.5876803398132324\n",
      "Epoch 14 - Train Loss: 3.8383662700653076 - Val Loss 3.558375120162964\n",
      "Epoch 15 - Train Loss: 3.8087832927703857 - Val Loss 3.529404640197754\n",
      "Epoch 16 - Train Loss: 3.7794318199157715 - Val Loss 3.502375364303589\n",
      "Epoch 17 - Train Loss: 3.750622034072876 - Val Loss 3.4769644737243652\n",
      "Epoch 18 - Train Loss: 3.721571922302246 - Val Loss 3.453150987625122\n",
      "Epoch 19 - Train Loss: 3.6907436847686768 - Val Loss 3.4327285289764404\n",
      "Epoch 20 - Train Loss: 3.659273862838745 - Val Loss 3.4129507541656494\n",
      "Epoch 21 - Train Loss: 3.6310369968414307 - Val Loss 3.3938159942626953\n",
      "Epoch 22 - Train Loss: 3.6028642654418945 - Val Loss 3.3759162425994873\n",
      "Epoch 23 - Train Loss: 3.574604034423828 - Val Loss 3.3586747646331787\n",
      "Epoch 24 - Train Loss: 3.545504331588745 - Val Loss 3.341212272644043\n",
      "Epoch 25 - Train Loss: 3.5172126293182373 - Val Loss 3.32470965385437\n",
      "Epoch 26 - Train Loss: 3.4898622035980225 - Val Loss 3.3094499111175537\n",
      "Epoch 27 - Train Loss: 3.4628288745880127 - Val Loss 3.293116807937622\n",
      "Epoch 28 - Train Loss: 3.4355251789093018 - Val Loss 3.2754812240600586\n",
      "Epoch 29 - Train Loss: 3.4082255363464355 - Val Loss 3.2558999061584473\n",
      "Epoch 30 - Train Loss: 3.380923271179199 - Val Loss 3.233579397201538\n",
      "Epoch 31 - Train Loss: 3.3538694381713867 - Val Loss 3.2092630863189697\n",
      "Epoch 32 - Train Loss: 3.3274271488189697 - Val Loss 3.184492349624634\n",
      "Epoch 33 - Train Loss: 3.3005259037017822 - Val Loss 3.1616578102111816\n",
      "Epoch 34 - Train Loss: 3.27360200881958 - Val Loss 3.142259120941162\n",
      "Epoch 35 - Train Loss: 3.2468976974487305 - Val Loss 3.1264383792877197\n",
      "Epoch 36 - Train Loss: 3.2201602458953857 - Val Loss 3.113487720489502\n",
      "Epoch 37 - Train Loss: 3.193411350250244 - Val Loss 3.1022796630859375\n",
      "Epoch 38 - Train Loss: 3.166865348815918 - Val Loss 3.092080593109131\n",
      "Epoch 39 - Train Loss: 3.1398086547851562 - Val Loss 3.0822031497955322\n",
      "Epoch 40 - Train Loss: 3.1126797199249268 - Val Loss 3.0719692707061768\n",
      "Epoch 41 - Train Loss: 3.085838556289673 - Val Loss 3.0593271255493164\n",
      "Epoch 42 - Train Loss: 3.058696746826172 - Val Loss 3.0447888374328613\n",
      "Epoch 43 - Train Loss: 3.0311930179595947 - Val Loss 3.0327930450439453\n",
      "Epoch 44 - Train Loss: 3.003622055053711 - Val Loss 3.025205373764038\n",
      "Epoch 45 - Train Loss: 2.97636079788208 - Val Loss 3.016080141067505\n",
      "Epoch 46 - Train Loss: 2.9496994018554688 - Val Loss 2.996933937072754\n",
      "Epoch 47 - Train Loss: 2.9230141639709473 - Val Loss 2.9724178314208984\n",
      "Epoch 48 - Train Loss: 2.8987483978271484 - Val Loss 2.9477882385253906\n",
      "Epoch 49 - Train Loss: 2.873816728591919 - Val Loss 2.9261186122894287\n",
      "Epoch 50 - Train Loss: 2.847512722015381 - Val Loss 2.9104208946228027\n",
      "Epoch 51 - Train Loss: 2.822911500930786 - Val Loss 2.8958580493927\n",
      "Epoch 52 - Train Loss: 2.797197103500366 - Val Loss 2.870851516723633\n",
      "Epoch 53 - Train Loss: 2.772257089614868 - Val Loss 2.8299875259399414\n",
      "Epoch 54 - Train Loss: 2.74662709236145 - Val Loss 2.7964208126068115\n",
      "Epoch 55 - Train Loss: 2.721383571624756 - Val Loss 2.7688045501708984\n",
      "Epoch 56 - Train Loss: 2.6966118812561035 - Val Loss 2.740877628326416\n",
      "Epoch 57 - Train Loss: 2.6723275184631348 - Val Loss 2.7120370864868164\n",
      "Epoch 58 - Train Loss: 2.6490097045898438 - Val Loss 2.667670488357544\n",
      "Epoch 59 - Train Loss: 2.6228294372558594 - Val Loss 2.6510798931121826\n",
      "Epoch 60 - Train Loss: 2.597860336303711 - Val Loss 2.625577926635742\n",
      "Epoch 61 - Train Loss: 2.5722973346710205 - Val Loss 2.6058595180511475\n",
      "Epoch 62 - Train Loss: 2.5487191677093506 - Val Loss 2.6099517345428467\n",
      "Epoch 63 - Train Loss: 2.523580312728882 - Val Loss 2.600165843963623\n",
      "Epoch 64 - Train Loss: 2.500577688217163 - Val Loss 2.543572187423706\n",
      "Epoch 65 - Train Loss: 2.4760918617248535 - Val Loss 2.526257276535034\n",
      "Epoch 66 - Train Loss: 2.451860189437866 - Val Loss 2.524704933166504\n",
      "Epoch 67 - Train Loss: 2.429495096206665 - Val Loss 2.4786036014556885\n",
      "Epoch 68 - Train Loss: 2.406402587890625 - Val Loss 2.4421987533569336\n",
      "Epoch 69 - Train Loss: 2.3831517696380615 - Val Loss 2.435957670211792\n",
      "Epoch 70 - Train Loss: 2.3584933280944824 - Val Loss 2.4113059043884277\n",
      "Epoch 71 - Train Loss: 2.3345253467559814 - Val Loss 2.346842050552368\n",
      "Epoch 72 - Train Loss: 2.3116812705993652 - Val Loss 2.321747303009033\n",
      "Epoch 73 - Train Loss: 2.2865328788757324 - Val Loss 2.3239481449127197\n",
      "Epoch 74 - Train Loss: 2.2612080574035645 - Val Loss 2.2913360595703125\n",
      "Epoch 75 - Train Loss: 2.2361066341400146 - Val Loss 2.234616994857788\n",
      "Epoch 76 - Train Loss: 2.2104694843292236 - Val Loss 2.2325599193573\n",
      "Epoch 77 - Train Loss: 2.1861984729766846 - Val Loss 2.2387800216674805\n",
      "Epoch 78 - Train Loss: 2.1635589599609375 - Val Loss 2.193499803543091\n",
      "Epoch 79 - Train Loss: 2.1415491104125977 - Val Loss 2.186318874359131\n",
      "Epoch 80 - Train Loss: 2.1208713054656982 - Val Loss 2.176506519317627\n",
      "Epoch 81 - Train Loss: 2.1006762981414795 - Val Loss 2.1350183486938477\n",
      "Epoch 82 - Train Loss: 2.0807783603668213 - Val Loss 2.10603928565979\n",
      "Epoch 83 - Train Loss: 2.0624783039093018 - Val Loss 2.1012449264526367\n",
      "Epoch 84 - Train Loss: 2.043762445449829 - Val Loss 2.091488838195801\n",
      "Epoch 85 - Train Loss: 2.0255703926086426 - Val Loss 2.0622682571411133\n",
      "Epoch 86 - Train Loss: 2.007852554321289 - Val Loss 2.0374460220336914\n",
      "Epoch 87 - Train Loss: 1.9900652170181274 - Val Loss 2.031522274017334\n",
      "Epoch 88 - Train Loss: 1.9719594717025757 - Val Loss 2.0137901306152344\n",
      "Epoch 89 - Train Loss: 1.95485520362854 - Val Loss 1.985358476638794\n",
      "Epoch 90 - Train Loss: 1.938385248184204 - Val Loss 1.9794511795043945\n",
      "Epoch 91 - Train Loss: 1.922985315322876 - Val Loss 1.982857346534729\n",
      "Epoch 92 - Train Loss: 1.9083002805709839 - Val Loss 1.9629846811294556\n",
      "Epoch 93 - Train Loss: 1.8943262100219727 - Val Loss 1.9445146322250366\n",
      "Epoch 94 - Train Loss: 1.8802608251571655 - Val Loss 1.9266645908355713\n",
      "Epoch 95 - Train Loss: 1.8665771484375 - Val Loss 1.941748857498169\n",
      "Epoch 96 - Train Loss: 1.853910207748413 - Val Loss 1.8865810632705688\n",
      "Epoch 97 - Train Loss: 1.8418539762496948 - Val Loss 1.9064205884933472\n",
      "Epoch 98 - Train Loss: 1.8351086378097534 - Val Loss 1.9306752681732178\n",
      "Epoch 99 - Train Loss: 1.829908013343811 - Val Loss 1.8578786849975586\n",
      "Epoch 100 - Train Loss: 1.8125317096710205 - Val Loss 1.8747178316116333\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    # convert the data to tensors\n",
    "    tensor_Xtr = torch.tensor(X_train.to_numpy(), dtype=torch.float)\n",
    "    tensor_Xva = torch.tensor(X_val.to_numpy(), dtype=torch.float)\n",
    "    tensor_ytr = torch.tensor(y_train.to_numpy(), dtype=torch.float)\n",
    "    tensor_yva = torch.tensor(y_val.to_numpy(), dtype=torch.float)\n",
    "\n",
    "    model.train()\n",
    "    outputs = model(tensor_Xtr)\n",
    "    loss = torch.sqrt(criterion(outputs, tensor_ytr))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_output = model(tensor_Xva)\n",
    "        val_loss = torch.sqrt(criterion(val_output, tensor_yva))\n",
    "        \n",
    "    print(f'Epoch {epoch+1} - Train Loss: {loss.item()} - Val Loss {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241af0f6-7c0f-4b87-a474-358aa50207a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
