{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1544cb55-4361-4e37-b775-3a6f4cdf564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from score import score\n",
    "from rec_utils import split_the_data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from surprise import SVDpp, BaselineOnly\n",
    "from surprise.model_selection import train_test_split as sur_tts\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da859d77-87e0-4fc0-9b39-f8e5f3a495f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(\"../Course_Scraper/assets/augumented_data/augmented_user_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2fb91f1-903d-4675-93e5-e313f996ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = {\n",
    "    k: x for x, k in enumerate(user_data['user_index'].unique())\n",
    "}\n",
    "\n",
    "# so we get a consistent titles\n",
    "title_mapping = {\n",
    "    k: x for x, k in enumerate(user_data['title_index'].unique())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2b03599-5f27-4200-8d9c-c9c84643d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "RMSE: 1.6152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.615152274194685"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(user_data, reader)\n",
    "train, test = sur_tts(data, test_size=0.2, shuffle=True)\n",
    "\n",
    "bsl_options = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"method\": \"sgd\",\n",
    "    \"learning_rate\": 0.001,\n",
    "}\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "algo.fit(train)\n",
    "predictions = algo.test(test)\n",
    "accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "918567ae-6542-49b7-9dc1-33d5f2df7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['user_index'] = user_data['user_index'].map(user_mapping)\n",
    "user_data['title_index'] = user_data['title_index'].map(title_mapping)\n",
    "\n",
    "data, test = split_the_data(user_data)\n",
    "train, val = split_the_data(data)\n",
    "\n",
    "X_train = train.drop('rating', axis=1)\n",
    "X_val = val.drop('rating', axis=1)\n",
    "\n",
    "y_train = train['rating']\n",
    "y_val = val['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cdf403c-804f-4f06-8819-4b8122916dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = len(user_mapping)\n",
    "total_titles = len(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f5f19b3-6586-466c-aef9-3e040e6ee950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(features, features)\n",
    "        self.bn1 = nn.BatchNorm1d(features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(features, features)\n",
    "        self.bn2 = nn.BatchNorm1d(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.fc1(x)))\n",
    "        out = self.bn2(self.fc2(out))\n",
    "        out += identity \n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c8954a9-687d-4def-82ee-37882cd93e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys(nn.Module):\n",
    "    def __init__(self, total_users, total_tiltes, user_emb_dim: int = 64, title_emb_dim: int = 64):\n",
    "        super(RecSys, self).__init__()\n",
    "        self.usr_emb = nn.Embedding(total_users, user_emb_dim)\n",
    "        self.title_emb = nn.Embedding(total_titles, title_emb_dim)\n",
    "        self.fc1 = nn.Linear((user_emb_dim + title_emb_dim), 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.res_block1 = ResidualBlock(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.res_block2 = ResidualBlock(64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        user = x[:, 1].long()\n",
    "        title = x[:, 0].long()\n",
    "\n",
    "        u = self.usr_emb(user)\n",
    "        t = self.title_emb(title)\n",
    "\n",
    "        x = torch.concat([u, t], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4a8d2ee-00bc-4ff4-a75e-a8d9439be2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecSys(total_users=total_users, total_tiltes=total_titles).to('cuda')\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-2, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15e8fd37-1e93-4617-828e-e112e0d60165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Loss: 1.687637209892273 - Val Loss 1.8616093397140503\n",
      "Epoch 40 - Train Loss: 1.6714032888412476 - Val Loss 1.7135950326919556\n",
      "Epoch 60 - Train Loss: 1.66849684715271 - Val Loss 1.7195463180541992\n",
      "Epoch 80 - Train Loss: 1.667236328125 - Val Loss 1.7242830991744995\n",
      "Epoch 100 - Train Loss: 1.6664929389953613 - Val Loss 1.7258421182632446\n",
      "Epoch 120 - Train Loss: 1.6659996509552002 - Val Loss 1.7265441417694092\n",
      "Epoch 140 - Train Loss: 1.6656428575515747 - Val Loss 1.726840853691101\n",
      "Epoch 160 - Train Loss: 1.6653741598129272 - Val Loss 1.7276394367218018\n",
      "Epoch 180 - Train Loss: 1.665173053741455 - Val Loss 1.7279800176620483\n",
      "Epoch 200 - Train Loss: 1.665021538734436 - Val Loss 1.7287442684173584\n",
      "Epoch 220 - Train Loss: 1.6649082899093628 - Val Loss 1.729594349861145\n",
      "Epoch 240 - Train Loss: 1.664820909500122 - Val Loss 1.7296056747436523\n",
      "Epoch 260 - Train Loss: 1.664750099182129 - Val Loss 1.729596495628357\n",
      "Epoch 280 - Train Loss: 1.6646968126296997 - Val Loss 1.729390263557434\n",
      "Epoch 300 - Train Loss: 1.664656639099121 - Val Loss 1.728792428970337\n",
      "Epoch 320 - Train Loss: 1.6646238565444946 - Val Loss 1.7274616956710815\n",
      "Epoch 340 - Train Loss: 1.6645936965942383 - Val Loss 1.7250014543533325\n",
      "Epoch 360 - Train Loss: 1.6645591259002686 - Val Loss 1.7205822467803955\n",
      "Epoch 380 - Train Loss: 1.6645193099975586 - Val Loss 1.7177902460098267\n",
      "Epoch 400 - Train Loss: 1.6644902229309082 - Val Loss 1.7168097496032715\n",
      "Epoch 420 - Train Loss: 1.664474606513977 - Val Loss 1.7168278694152832\n",
      "Epoch 440 - Train Loss: 1.664466381072998 - Val Loss 1.7169278860092163\n",
      "Epoch 460 - Train Loss: 1.6644631624221802 - Val Loss 1.7168121337890625\n",
      "Epoch 480 - Train Loss: 1.6644614934921265 - Val Loss 1.7170839309692383\n",
      "Epoch 500 - Train Loss: 1.664459228515625 - Val Loss 1.7186673879623413\n",
      "Epoch 520 - Train Loss: 1.6644575595855713 - Val Loss 1.721242904663086\n",
      "Epoch 540 - Train Loss: 1.6644561290740967 - Val Loss 1.7230644226074219\n",
      "Epoch 560 - Train Loss: 1.6644561290740967 - Val Loss 1.7226126194000244\n",
      "Epoch 580 - Train Loss: 1.6644560098648071 - Val Loss 1.7217440605163574\n",
      "Epoch 600 - Train Loss: 1.6644554138183594 - Val Loss 1.720870852470398\n",
      "Epoch 620 - Train Loss: 1.6644551753997803 - Val Loss 1.7200703620910645\n",
      "Epoch 640 - Train Loss: 1.6644549369812012 - Val Loss 1.7194582223892212\n",
      "Epoch 660 - Train Loss: 1.6644548177719116 - Val Loss 1.7190288305282593\n",
      "Epoch 680 - Train Loss: 1.6644545793533325 - Val Loss 1.7187365293502808\n",
      "Epoch 700 - Train Loss: 1.664454460144043 - Val Loss 1.7185401916503906\n",
      "Epoch 720 - Train Loss: 1.6644543409347534 - Val Loss 1.7184092998504639\n",
      "Epoch 740 - Train Loss: 1.6644542217254639 - Val Loss 1.71832275390625\n",
      "Epoch 760 - Train Loss: 1.6644541025161743 - Val Loss 1.718266487121582\n",
      "Epoch 780 - Train Loss: 1.6644539833068848 - Val Loss 1.7182308435440063\n",
      "Epoch 800 - Train Loss: 1.6644538640975952 - Val Loss 1.718208909034729\n",
      "Epoch 820 - Train Loss: 1.6644538640975952 - Val Loss 1.7181956768035889\n",
      "Epoch 840 - Train Loss: 1.6644536256790161 - Val Loss 1.7181884050369263\n",
      "Epoch 860 - Train Loss: 1.6644536256790161 - Val Loss 1.7181848287582397\n",
      "Epoch 880 - Train Loss: 1.6644536256790161 - Val Loss 1.7181836366653442\n",
      "Epoch 900 - Train Loss: 1.6644535064697266 - Val Loss 1.7181837558746338\n",
      "Epoch 920 - Train Loss: 1.6644535064697266 - Val Loss 1.7181848287582397\n",
      "Epoch 940 - Train Loss: 1.6644535064697266 - Val Loss 1.7181862592697144\n",
      "Epoch 960 - Train Loss: 1.6644532680511475 - Val Loss 1.718187928199768\n",
      "Epoch 980 - Train Loss: 1.6644532680511475 - Val Loss 1.7181897163391113\n",
      "Epoch 1000 - Train Loss: 1.6644532680511475 - Val Loss 1.718191385269165\n",
      "Epoch 1020 - Train Loss: 1.664453148841858 - Val Loss 1.7181930541992188\n",
      "Epoch 1040 - Train Loss: 1.664453148841858 - Val Loss 1.718194603919983\n",
      "Epoch 1060 - Train Loss: 1.664453148841858 - Val Loss 1.7181960344314575\n",
      "Epoch 1080 - Train Loss: 1.664453148841858 - Val Loss 1.7181973457336426\n",
      "Epoch 1100 - Train Loss: 1.664453148841858 - Val Loss 1.7181986570358276\n",
      "Epoch 1120 - Train Loss: 1.6644529104232788 - Val Loss 1.7181998491287231\n",
      "Epoch 1140 - Train Loss: 1.6644529104232788 - Val Loss 1.718200922012329\n",
      "Epoch 1160 - Train Loss: 1.6644529104232788 - Val Loss 1.718201756477356\n",
      "Epoch 1180 - Train Loss: 1.6644529104232788 - Val Loss 1.7182025909423828\n",
      "Epoch 1200 - Train Loss: 1.6644529104232788 - Val Loss 1.7182034254074097\n",
      "Epoch 1220 - Train Loss: 1.6644529104232788 - Val Loss 1.718204140663147\n",
      "Epoch 1240 - Train Loss: 1.6644527912139893 - Val Loss 1.7182047367095947\n",
      "Epoch 1260 - Train Loss: 1.6644527912139893 - Val Loss 1.7182055711746216\n",
      "Epoch 1280 - Train Loss: 1.6644527912139893 - Val Loss 1.7182060480117798\n",
      "Epoch 1300 - Train Loss: 1.6644527912139893 - Val Loss 1.7182066440582275\n",
      "Epoch 1320 - Train Loss: 1.6644526720046997 - Val Loss 1.7182071208953857\n",
      "Epoch 1340 - Train Loss: 1.6644527912139893 - Val Loss 1.7182077169418335\n",
      "Epoch 1360 - Train Loss: 1.6644526720046997 - Val Loss 1.7182080745697021\n",
      "Epoch 1380 - Train Loss: 1.6644526720046997 - Val Loss 1.7182085514068604\n",
      "Epoch 1400 - Train Loss: 1.6644525527954102 - Val Loss 1.718208909034729\n",
      "Epoch 1420 - Train Loss: 1.6644526720046997 - Val Loss 1.7182092666625977\n",
      "Epoch 1440 - Train Loss: 1.6644524335861206 - Val Loss 1.7182095050811768\n",
      "Epoch 1460 - Train Loss: 1.6644526720046997 - Val Loss 1.7182098627090454\n",
      "Epoch 1480 - Train Loss: 1.6644525527954102 - Val Loss 1.7182103395462036\n",
      "Epoch 1500 - Train Loss: 1.6644529104232788 - Val Loss 1.7182106971740723\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1500):\n",
    "    # convert the data to tensors\n",
    "    tensor_Xtr = torch.tensor(X_train.to_numpy(), dtype=torch.float).to('cuda')\n",
    "    tensor_Xva = torch.tensor(X_val.to_numpy(), dtype=torch.float).to('cuda')\n",
    "    tensor_ytr = torch.tensor(y_train.to_numpy(), dtype=torch.float).to('cuda')\n",
    "    tensor_yva = torch.tensor(y_val.to_numpy(), dtype=torch.float).to('cuda')\n",
    "\n",
    "    model.train()\n",
    "    outputs = model(tensor_Xtr)\n",
    "    loss = torch.sqrt(criterion(outputs, tensor_ytr)).to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_output = model(tensor_Xva).to('cuda')\n",
    "        val_loss = torch.sqrt(criterion(val_output, tensor_yva)).to('cuda')\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1} - Train Loss: {loss.item()} - Val Loss {val_loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
